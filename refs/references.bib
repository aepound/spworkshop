% This file was created with JabRef 2.5.
% Encoding: UTF-8

@INPROCEEDINGS{Crawford2003, 
author={Crawford, M.M. and Jisoo Ham and Yangchi Chen and Ghosh, J.}, 
booktitle={Advances in Techniques for Analysis of Remotely Sensed Data, 2003 IEEE Workshop on}, 
title={Random forests of binary hierarchical classifiers for analysis of hyperspectral data}, 
year={2003}, 
month={Oct}, 
pages={337-345}, 
keywords={data analysis;feature extraction;image
classification;spectral analysis;statistical distributions;trees
(mathematics);Botswana;CART classifier;Hyperion sensor;NASA EO-1
satellite;Okavango Delta;adaptive random subspace feature
selection;associated training data;binary hierarchical classifier
algorithm;binary hierarchy;class
distributions;generalization;hyperspectral data
analysis;multiclassifier system;random forests;statistical
classification;trees;Classification tree analysis;Covariance
matrix;Data analysis;Feature extraction;Hyperspectral
imaging;Hyperspectral sensors;Instruments;NASA;Satellites;Training
data},  
doi={10.1109/WARSD.2003.1295213},}                  
                  
@book{Breiman1984,
  title={Classification and Regression Trees},
  author={Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  isbn={9780412048418},
  lccn={83019708},
  series={The Wadsworth and Brooks-Cole statistics-probability series},
  year={1984},
  publisher={Taylor \& Francis}
}
                         
@INPROCEEDINGS{GomezChova2003, 
author={Gomez-Chova, L. and Calpe, J. and Soria, E. and Camps-Valls,
G. and Martin, J.D. and Moreno, J.},  
booktitle={Image Processing, 2003. ICIP 2003. Proceedings. 2003
International Conference on},  
title={CART-based feature selection of hyperspectral images for crop
cover classification},  
year={2003}, 
month={Sept}, 
volume={3}, 
keywords={feature extraction;image classification;vegetation
mapping;classification and regression trees;crop cover
classification;data dimensionality;dimensionality reduction
strategy;feature selection;hyperspectral images;knowledge
discovery;pattern recognition techniques;Classification tree
analysis;Crops;Hyperspectral imaging;Image processing;Image
recognition;Information analysis;Pattern
recognition;Proposals;Regression tree analysis;Testing},  
doi={10.1109/ICIP.2003.1247313}, 
ISSN={1522-4880},}                  
                  
@book{Moon2000,
  title={Mathematical Methods and Algorithms for Signal Processing},
  author={Moon, T.K. and Stirling, W.C.},
  isbn={9780201361865},
  lccn={99031038},
  year={2000},
  publisher={Prentice Hall}
}
                  
@ARTICLE{Bandos2009, 
author={Bandos, T.V. and Bruzzone, L. and Camps-Valls, G.}, 
journal={Geoscience and Remote Sensing, IEEE Transactions on}, 
title={Classification of Hyperspectral Images With Regularized Linear
Discriminant Analysis},  
year={2009}, 
month={March}, 
volume={47}, 
number={3}, 
pages={862-873}, 
keywords={geophysical techniques;image classification;remote
sensing;support vector machines;RLDA technique;SVM;hyperspectral
image;ill-posed problem;image classification;regularized linear
discriminant analysis;remote sensing;support-
vector-machine;Hyperspectral imaging;Hyperspectral sensors;Image
analysis;Image classification;Kernel;Linear discriminant
analysis;Problem-solving;Remote sensing;Support vector machine
classification;Support vector machines;Hyperspectral images;ill-posed
problem;image classification;linear discriminant analysis
(LDA);regularization},  
doi={10.1109/TGRS.2008.2005729}, 
ISSN={0196-2892}}                  
                  
@incollection{Du2008,
year={2008},
isbn={978-3-540-85566-8},
booktitle={Knowledge-Based Intelligent Information and Engineering Systems},
volume={5179},
series={Lecture Notes in Computer Science},
editor={Lovrek, Ignac and Howlett, RobertJ. and Jain, LakhmiC.},
doi={10.1007/978-3-540-85567-5_49},
title={Dimensionality Reduction and Linear Discriminant Analysis for
       Hyperspectral Image Classification}, 
publisher={Springer Berlin Heidelberg},
keywords={Fisher's Linear Discriminant Analysis; Dimensionality
Reduction; Classification; Hyperspectral Imagery}, 
author={Du, Qian and Younan, NicolasH.},
pages={392-399},
language={English}
}                  
                  
@book{Murphy2012,
  title={Machine Learning: A Probabilistic Perspective},
  author={Murphy, K.P.},
  isbn={9780262018029},
  lccn={2012004558},
  series={Adaptive computation and machine learning series},
  year={2012},
  publisher={MIT Press}
}

%% Original references.bib:
                  
@ARTICLE{Aharon2006,
  author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
  title = {K-SVD: an Algorithm for Designing Overcomplete Dictionaries for
	Sparse Representation},
  journal = {IEEE Transactions on Signal Processing},
  year = {2006},
  volume = {54},
  pages = {4311--4322},
  number = {11},
  abstract = {In recent years there has been a growing interest in the study of
	sparse representation of signals. Using an overcomplete dictionary
	that contains prototype signal-atoms, signals are described by sparse
	linear combinations of these atoms. Applications that use sparse
	representation are many and include compression, regularization in
	inverse problems, feature extraction, and more. Recent activity in
	this field has concentrated mainly on the study of pursuit algorithms
	that decompose signals with respect to a given dictionary. Designing
	dictionaries to better fit the above model can be done by either
	selecting one from a prespecified set of linear transforms or adapting
	the dictionary to a set of training signals. Both of these techniques
	have been considered, but this topic is largely still open. In this
	paper we propose a novel algorithm for adapting dictionaries in order
	to achieve sparse signal representations. Given a set of training
	signals, we seek the dictionary that leads to the best representation
	for each member in this set, under strict sparsity constraints. We
	present a new method-the K-SVD algorithm-generalizing the K-means
	clustering process. K-SVD is an iterative method that alternates
	between sparse coding of the examples based on the current dictionary
	and a process of updating the dictionary atoms to better fit the
	data. The update of the dictionary columns is combined with an update
	of the sparse representations, thereby accelerating convergence.
	The K-SVD algorithm is flexible and can work with any pursuit method
	(e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this
	algorithm and demonstrate its results both on synthetic tests and
	in applications on real image data},
  doi = {10.1109/TSP.2006.881199},
  file = {:home/aepound/research/overcomplete/refs/toread/2006-K-SVD-Aharon.pdf:pdf},
  keywords = {K-SVD,K-means clustering process,image coding,image data,image representation,iterative
	method,iterative methods,linear transforms,overcomplete dictionary,signals
	sparse representation,singular value decomposition,sparse coding,sparsity
	constraints,transforms},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=1710377}
}

@article{Bayliss1998,
author = {Bayliss, Jessica D. and Gualtieri, J. Anthony and Cromp, Robert F.},
title = {Analyzing hyperspectral data with independent component analysis},
journal = {Proceedinds of the SPIE},
volume = {3240},
number = {},
pages = {133--143},
abstract = {Hyperspectral image sensors provide images with a large
number of contiguous spectral channels per pixel and enable
information about different materials within a pixel to be
obtained. The problem of spectrally unmixing materials may be viewed
as a specific case of the blind source separation problem where data
consists of mixed signals and the goal is to determine the
contribution of each mineral to the mix without prior knowledge of the
minerals in the mix. The technique of independent component analysis
(ICA) assumes that the spectral components are close to statistically
independent and provides an unsupervised method for blind source
separation. We introduce contextual ICA in the context of
hyperspectral data analysis and apply the method to mineral data from
synthetically mixed minerals and real image signatures.}, 
year = {1998},
doi = {10.1117/12.300050},
optURL = { http://dx.doi.org/10.1117/12.300050},
eprint = {}
} 
				  
@article{BenDor2009,
title = "Using Imaging Spectroscopy to study soil properties ",
journal = "Remote Sensing of Environment ",
volume = "113, Supplement 1",
number = "0",
pages = {S38--S55},
year = "2009",
note = "Imaging Spectroscopy Special Issue ",
issn = "0034-4257",
doi = "http://dx.doi.org/10.1016/j.rse.2008.09.019",
opturl = "http://www.sciencedirect.com/science/article/pii/S0034425709000753",
author = "E. Ben-Dor and S. Chabrillat and J.A.M. DemattÃª and G.R. Taylor and J. Hill and M.L. Whiting and S. Sommer",
keywords = "Soil reflectance",
keywords = "Image spectroscopy",
keywords = "Soil properties",
keywords = "Soil applications "
}

@INPROCEEDINGS{Bidhendi2007,
  author = {Bidhendi, S.K. and Shirazi, A.S. and Fotoohi, N. and Ebadzadeh, M.M.},
  title = {Material Classification of Hyperspectral Images Using Unsupervised
	Fuzzy Clustering Methods},
  booktitle = {Third International IEEE Conference on Signal-Image
  Technologies and Internet-Based System },
  year = {2007},
  OPTpages = {619--623},
  month = dec,
  __markedentry = {[aepound:6]},
  abstract = {This paper presents a novel approach in classifying materials in Hyperspectral
	images. In particular, unlike other similar approaches in which every
	pixel in the image is mapped to one of the reference spectra, the
	proposed methods use the data itself to create clusters of pixels
	with the same material. This is done by using unsupervised fuzzy
	clustering methods. Here, two fuzzy clustering approaches have been
	addressed: Fuzzy C-Means clustering (FCM) and fuzzy relational clustering
	(FRC). The proposed methods can also solve the problem of identifying
	the objects for which the radiance of light makes it barely hard
	to identify them as a single object e.g., a pitched roof. The proposed
	methods have been applied on the CASI image and the results show
	that they can successfully classify the materials in the image.},
  doi = {10.1109/SITIS.2007.113},
  keywords = {CASI image;fuzzy c-means clustering;fuzzy relational clustering;hyperspectral
	images;image pixel mapping;light radiance;material classification;object
	identification;pixel clusters;reference spectra;unsupervised fuzzy
	clustering method;fuzzy set theory;image classification;pattern clustering;spectral
	analysis;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@INBOOK{Boyd2004,
  chapter = {6) Approxi},
  pages = {304--305, 333--337},
  title = {{Sparse Descriptions and Basis Pursuit}},
  publisher = {Cambridge University Press},
  year = {2004},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  address = {New York, New York, USA},
  edition = {Seventh},
  booktitle = {Convex Optimization},
  isbn = {0 521 83378 7},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://www.stanford.edu/\~{}boyd/cvxbook/}
}

@BOOK{BoydCvxBook,
  title = {{Convex Optimization}},
  publisher = {Cambridge University Press},
  year = {2004},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  month = mar,
  abstract = {{Convex optimization problems arise frequently in many different fields.
	A comprehensive introduction to the subject, this book shows in detail
	how such problems can be solved numerically with great efficiency.
	The focus is on recognizing convex optimization problems and then
	finding the most appropriate technique for solving them. The text
	contains many worked examples and homework exercises and will appeal
	to students, researchers and practitioners in fields such as engineering,
	computer science, mathematics, statistics, finance, and economics.}},
  citeulike-article-id = {163662},
  citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0521833787},
  citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0521833787},
  citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0521833787},
  citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0521833787},
  citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0521833787/citeulike00-21},
  citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0521833787},
  citeulike-linkout-6 = {http://www.worldcat.org/isbn/0521833787},
  citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0521833787},
  citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0521833787\&index=books\&linkCode=qs},
  citeulike-linkout-9 = {http://www.librarything.com/isbn/0521833787},
  comment = {Code, PDF here: http://www.stanford.edu/\~{}boyd/cvxbook/},
  day = {08},
  howpublished = {Hardcover},
  isbn = {9780521833783},
  keywords = {machine\_learning},
  owner = {aepound},
  posted-at = {2008-03-13 01:30:18},
  priority = {2},
  timestamp = {2012.02.07},
  opturl = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0521833787}
}

@ARTICLE{Breiman2001,
  author = {Breiman, Leo},
  title = {Random Forests},
  journal = {Machine Learning},
  year = {2001},
  volume = {45},
  pages = {5--32},
  OPTnote = {10.1023/A:1010933404324},
  issn = {0885-6125},
  issue = {1},
  keyword = {Computer Science},
  owner = {aepound},
  publisher = {Springer Netherlands},
  timestamp = {2012.04.30},
  opturl = {http://dx.doi.org/10.1023/A:1010933404324}
}

@INPROCEEDINGS{Caruana2008,
  author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
  title = {An empirical evaluation of supervised learning in high dimensions},
  booktitle = {Proceedings of the 25th international conference on Machine learning},
  year = {2008},
  series = {ICML '08},
  pages = {96--103},
  address = {New York, NY, USA},
  publisher = {ACM},
  __markedentry = {[aepound]},
  acmid = {1390169},
  doi = {10.1145/1390156.1390169},
  isbn = {978-1-60558-205-4},
  location = {Helsinki, Finland},
  numpages = {8},
  owner = {aepound},
  timestamp = {2012.05.01},
  opturl = {http://doi.acm.org/10.1145/1390156.1390169}
}

@INPROCEEDINGS{Castrodad2010,
  author = {Castrodad, Alexey and Xing, Zhengming and Greer, John and Bosch,
	Edward and Carin, Lawrence and Sapiro, Guillermo},
  title = {{Discriminative sparse representations in hyperspectral imagery}},
  booktitle = {2010 IEEE International Conference on Image Processing},
  year = {2010},
  OPTpages = {1313--1316},
  month = sep,
  publisher = {IEEE},
  abstract = {Recent advances in sparse modeling and dictionary learning for discriminative
	applications show high potential for numerous classification tasks.
	In this paper, we show that highly accurate material classification
	from hyperspectral imagery (HSI) can be obtained with these models,
	even when the data is reconstructed from a very small percentage
	of the original image samples. The proposed supervised HSI classification
	is performed using a measure that accounts for both reconstruction
	errors and sparsity levels for sparse representations based on class-dependent
	learned dictionaries. Combining the dictionaries learned for the
	different materials, a linear mixing model is derived for sub-pixel
	classification. Results with real hyperspectral data cubes are shown
	both for urban and non-urban terrain.},
  doi = {10.1109/ICIP.2010.5651568},
  file = {:home/aepound/research/overcomplete/hsi/2010-DiscrimSparseRepsHSI-Castrodad.pdf:pdf},
  hsibands = {VNIR-SWIR (HyDICE)},
  hsiframework = {Bayesian framework},
  hsinotes = {Basically Classification by Sparse representation.},
  isbn = {978-1-4244-7992-4},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5651568}
}

@article{Chabrillat2002,
title = "Use of hyperspectral images in the identification and mapping of expansive clay soils and the role of spatial resolution ",
journal = "Remote Sensing of Environment ",
volume = "82",
number = "2â3",
pages = {431--445},
year = "2002",
note = "",
issn = "0034-4257",
doi = "http://dx.doi.org/10.1016/S0034-4257(02)00060-3",
opturl = "http://www.sciencedirect.com/science/article/pii/S0034425702000603",
author = "Sabine Chabrillat and Alexander F.H Goetz and Lisa Krosley and Harold W Olsen"
}

@INPROCEEDINGS{Chandrasekaran2009,
  author = {Chandrasekaran, V. and Sanghavi, S. and Parrilo, P.A. and Willsky,
	A.S.},
  title = {Sparse and low-rank matrix decompositions},
  booktitle = {Communication, Control, and Computing, 2009. Allerton 2009. 47th
	Annual Allerton Conference on},
  year = {2009},
  pages = {962--967},
  month = oct,
  doi = {10.1109/ALLERTON.2009.5394889},
  keywords = {NP-hard problem;convex optimization;ill-posed;low rank matrix decomposition;notion
	quantification;sparse matrix;matrix decomposition;optimisation;sparse
	matrices;},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@BOOK{Charles2010,
  title = {{Sparse coding for spectral signatures in hyperspectral images}},
  publisher = {IEEE},
  year = {2010},
  author = {Charles, Adam and Olshausen, Bruno and Rozell, Christopher J},
  pages = {191--195},
  month = nov,
  abstract = {The growing use of hyperspectral imagery lead us to seek automated
	algorithms for extracting useful information about the scene. Recent
	work in sparse approximation has shown that unsupervised learning
	techniques can use example data to determine an efficient dictionary
	with few a priori assumptions. We apply this model to sample hyperspectral
	data and show that these techniques learn a dictionary that: 1) contains
	a meaningful spectral decomposition for hyperspectral imagery, 2)
	admit representations that are useful in determining properties and
	classifying materials in the scene, and 3) forms local approximations
	to the nonlinear manifold structure present in the actual data.},
  booktitle = {2010 Conference Record of the Forty Fourth Asilomar Conference on
	Signals, Systems and Computers},
  doi = {10.1109/ACSSC.2010.5757496},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Charles, Olshausen, Rozell - 2010 - Sparse coding for spectral signatures in hyperspectral images.pdf:pdf},
  hsibands = {VNIR-SWIR (PROBE2)},
  hsinotes = { Geological focus, Demonstrates usefulness of dictionaries and shows
	that they produce results corresponding well with physical results.},
  isbn = {978-1-4244-9722-5},
  keywords = {Approximation methods,Dictionaries,Hyperspectral imaging,Manifolds,Materials,Pixel},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5757496}
}

@ARTICLE{Charles2011,
  author = {Charles, Adam S and Olshausen, Bruno A and Rozell, Christopher J},
  title = {Learning Sparse Codes for Hyperspectral Imagery},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  year = {2011},
  volume = {5},
  pages = {963--978},
  number = {5},
  month = sep,
  abstract = {The spectral features in hyperspectral imagery (HSI) contain significant
	structure that, if properly characterized, could enable more efficient
	data acquisition and improved data analysis. Because most pixels
	contain reflectances of just a few materials, we propose that a sparse
	coding model is well-matched to HSI data. Sparsity models consider
	each pixel as a combination of just a few elements from a larger
	dictionary, and this approach has proven effective in a wide range
	of applications. Furthermore, previous work has shown that optimal
	sparse coding dictionaries can be learned from a dataset with no
	other a priori information (in contrast to many HSI endmember discovery
	algorithms that assume the presence of pure spectra or side information).
	We modified an existing unsupervised learning approach and applied
	it to HSI data (with significant ground truth labeling) to learn
	an optimal sparse coding dictionary. Using this learned dictionary,
	we demonstrate three main findings: i) the sparse coding model learns
	spectral signatures of materials in the scene and locally approximates
	nonlinear manifolds for individual materials, ii) this learned dictionary
	can be used to infer HSI-resolution data with very high accuracy
	from simulated imagery collected at multispectral-level resolution,
	and iii) this learned dictionary improves the performance of a supervised
	classification algorithm, both in terms of the classifier complexity
	and generalization from very small training sets.},
  doi = {10.1109/JSTSP.2011.2149497},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Charles, Olshausen, Rozell - 2011 - Learning Sparse Codes for Hyperspectral Imagery.pdf:pdf},
  issn = {1932-4553},
  keywords = {Deblurring,Dictionary Learning,Hyperspectral Imagery,Inverse Problems,Material
	Clasification,Multispectral Imagery,Remote Sensing,Sparse coding},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5762314}
}

@ARTICLE{Chen2001,
  author = {Scott Shaobing Chen and David L. Donoho and Michael A. Saunders},
  title = {Atomic Decomposition by Basis Pursuit},
  journal = {SIAM Review},
  year = {2001},
  volume = {43},
  pages = {129--159},
  number = {1},
  coden = {SIREAD},
  doi = {DOI:10.1137/S003614450037906X},
  eissn = {10957200},
  issn = {00361445},
  keywords = {overcomplete signal representation; interior-point methods for linear
	programming; total variation denoising; multiscale edges; MATLAB
	code; denoising; time-frequency analysis; time-scale analysis; $\ell^1$
	norm optimization; matching pursuit; wavelets; wavelet packets; cosine
	packets; 94A12; 65K05; 65D15; 41A45; },
  owner = {aepound},
  publisher = {SIAM},
  timestamp = {2012.02.07},
  opturl = {http://dx.doi.org/10.1137/S003614450037906X}
}

@ARTICLE{Chen2011,
  author = {Chen, Yi and Nasrabadi, Nasser M and Tran, Trac D},
  title = {Sparse Representation for Target Detection in Hyperspectral Imagery},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  year = {2011},
  volume = {5},
  pages = {629--640},
  number = {3},
  month = jun,
  abstract = {In this paper, we propose a new sparsity-based algorithm for automatic
	target detection in hyperspectral imagery (HSI). This algorithm is
	based on the concept that a pixel in HSI lies in a low-dimensional
	subspace and thus can be represented as a sparse linear combination
	of the training samples. The sparse representation (a sparse vector
	corresponding to the linear combination of a few selected training
	samples) of a test sample can be recovered by solving an l0-norm
	minimization problem. With the recent development of the compressed
	sensing theory, such minimization problem can be recast as a standard
	linear programming problem or efficiently approximated by greedy
	pursuit algorithms. Once the sparse vector is obtained, the class
	of the test sample can be determined by the characteristics of the
	sparse vector on reconstruction. In addition to the constraints on
	sparsity and reconstruction accuracy, we also exploit the fact that
	in HSI the neighboring pixels have a similar spectral characteristic
	(smoothness). In our proposed algorithm, a smoothness constraint
	is also imposed by forcing the vector Laplacian at each reconstructed
	pixel to be minimum all the time within the minimization process.
	The proposed sparsity-based algorithm is applied to several hyperspectral
	imagery to detect targets of interest. Simulation results show that
	our algorithm outperforms the classical hyperspectral target detection
	algorithms, such as the popular spectral matched filters, matched
	subspace detectors, adaptive subspace detectors, as well as binary
	classifiers such as support vector machines.},
  doi = {10.1109/JSTSP.2011.2113170},
  file = {:home/aepound/research/overcomplete/hsi/2011-SparseTargetDetection-Chen.pdf:pdf},
  issn = {1932-4553},
  keywords = {Detectors,Dictionaries,Kernel,Object detection,Pixel,Support vector
	machines,Training},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5711635}
}

@ARTICLE{Delicado2001,
  author = {Delicado, P},
  title = {{Another Look at Principal Curves and Surfaces}},
  journal = {Journal of Multivariate Analysis},
  year = {2001},
  volume = {77},
  pages = {84--116},
  number = {1},
  doi = {10.1006/jmva.2000.1917},
  file = {:home/aepound/research/dimred/refs/delicado99another.pdf:pdf;:home/aepound/research/dimred/refs/delicadopc.pdf:pdf},
  issn = {0047259X},
  keywords = {fixed points,generalized total variance,nonlinear multi,phrases,principal
	components,smoothing techniques,variate analysis},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://linkinghub.elsevier.com/retrieve/pii/S0047259X00919171}
}

@ARTICLE{Demir2007,
  author = {Demir, B. and Erturk, S.},
  title = {Hyperspectral Image Classification Using Relevance Vector Machines},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  year = {2007},
  volume = {4},
  pages = {586--590},
  number = {4},
  month = oct,
  __markedentry = {[aepound:6]},
  abstract = {This letter presents a hyperspectral image classification method based
	on relevance vector machines (RVMs). Support vector machine (SVM)-based
	approaches have been recently proposed for hyperspectral image classification
	and have raised important interest. In this letter, it is genuinely
	proposed to use an RVM-based approach for the classification of hyperspectral
	images. It is shown that approximately the same classification accuracy
	is obtained using RVM-based classification, with a significantly
	smaller relevance vector rate and, therefore, much faster testing
	time, compared with SVM-based classification. This feature makes
	the RVM-based hyperspectral classification approach more suitable
	for applications that require low complexity and, possibly, real-time
	classification.},
  doi = {10.1109/LGRS.2007.903069},
  issn = {1545-598X},
  keywords = {hyperspectral image classification;relevance vector machines;remote
	sensing;support vector machine;geophysical signal processing;geophysical
	techniques;image classification;remote sensing;support vector machines;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@INPROCEEDINGS{Du2003,
  author = {Du, Q. and Chakrarvarty, S.},
  title = {Unsupervised hyperspectral image classification using blind source
	separation},
  booktitle = {Proceedings of the IEEE International Conference on Acoustics, Speech, and
  Signal Processing },
  year = {2003},
  volume = {3},
  OPTpages = {437--440},
  month = apr,
  __markedentry = {[aepound:6]},
  abstract = { This paper presents an unsupervised classification algorithm for
	hyperspectral remotely sensed imagery based on blind source separation.
	Since the area covered by a single pixel in such an image is very
	large, the reflectance of a pixel is the mixture from all the materials
	resident in this area. A contrast function consisting of the mutual
	information minimization and orthogonality among the outputs, is
	defined to separate the assumed linear mixture so as to achieve soft
	classification. In order to reduce the computational complexity,
	a Neyman-Pearson detection theory based eigen-thresholding method
	is used to estimate the number of classes, followed by a band selection
	technique to select a smaller number of bands used in the learning
	algorithm. The preliminary result using an AVIRIS experiment demonstrates
	the feasibility of the proposed algorithm.},
  doi = {10.1109/ICASSP.2003.1199505},
  issn = {1520-6149},
  keywords = { AVIRIS experiment; Neyman-Pearson detection theory; band selection
	technique; blind source separation; class estimation; computational
	complexity reduction; contrast function; eigen-thresholding method;
	mutual information minimization; orthogonality; pixel reflectance;
	remotely sensed imagery; soft classification; unsupervised hyperspectral
	image classification; blind source separation; eigenvalues and eigenfunctions;
	image classification; reflectivity; remote sensing;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@ARTICLE{Efron2004,
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani,
	Robert},
  title = {{Least Angle Regression}},
  journal = {The Annals of Statistics},
  year = {2004},
  volume = {32},
  pages = {407--451},
  number = {4},
  month = jun,
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward
	Selection and Backward Elimination is to choose a linear model on
	the basis of the same set of data to which the model will be applied.
	Typically we have available a large collection of possible covariates
	from which we hope to select a parsimonious set for the efficient
	prediction of a response variable. Least Angle Regression (LARS),
	a new model selection algorithm, is a useful and less greedy version
	of traditional forward selection methods. Three main properties are
	derived: (1) A simple modification of the LARS algorithm implements
	the Lasso, an attractive version of ordinary least squares that constrains
	the sum of the absolute regression coefficients; the LARS modification
	calculates all possible Lasso estimates for a given problem, using
	an order of magnitude less computer time than previous methods. (2)
	A different LARS modification efficiently implements Forward Stagewise
	linear regression, another promising new model selection method;},
  arxivid = {math/0406456},
  doi = {10.1016/j.neuroimage.2010.05.017},
  file = {:home/aepound/research/overcomplete/refs/toread/2004-LeastAngleRegression\_1-EfronHastie.pdf:pdf;:home/aepound/research/overcomplete/refs/toread/2004-LeastAngleRegression\_2-EfronHastie.pdf:pdf;:home/aepound/research/overcomplete/refs/toread/2004-LeastAngleRegression\_0-EfronHastie.pdf:pdf},
  issn = {1095-9572},
  keywords = {Lasso,Statistics,Theory,boosting,coefficient paths,linear regression,variable
	selection},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://arxiv.org/abs/math/0406456}
}

@ARTICLE{Einbeck2010,
  author = {Einbeck, J and Evers, L and Powell, B},
  title = {{Data Compression and Regression Through Local Principal Curves and
	Surfaces}},
  journal = {International Journal of Neural Systems},
  year = {2010},
  volume = {20},
  pages = {177--192},
  number = {3},
  abstract = {We consider principal curves and surfaces in the context of multivariate
	regression modelling. For predictor spaces featuring complex dependency
	patterns between the involved variables, the intrinsic dimensionality
	of the data tends to be very small due to the high redundancy induced
	by the dependencies. In situations of this type, it is useful to
	approximate the high-dimensional predictor space through a low-dimensional
	manifold (i.e., a curve or a surface), and use the projections onto
	the manifold as compressed predictors in the regression problem.
	In the case that the intrinsic dimensionality of the predictor space
	equals one, we use the local principal curve algorithm for the the
	compression step. We provide a novel algorithm which extends this
	idea to local principal surfaces, thus covering cases of an intrinsic
	dimensionality equal to two, which is in principle extendible to
	manifolds of arbitrary dimension. We motivate and apply the novel
	techniques using astrophysical and oceanographic data examples},
  institution = {Department of Mathematical Sciences, Durham University, Durham DH1
	3LE, England. jochen.einbeck@durham.ac.uk},
  owner = {aepound},
  timestamp = {2012.02.07},
  url =          {http://dx.doi.org/10.1142/S0129065710002346}
}

@ARTICLE{Einbeck2005,
  author = {Einbeck, J and Tutz, G and Evers, L},
  title = {{Local principal curves.}},
  journal = {Statistics and Computing},
  year = {2005},
  volume = {15},
  pages = {301--313},
  number = {4},
  abstract = {Principal components are a well established tool in dimension reduction.
	The extension to principal curves allows for general smooth curves
	which pass through the middle of a multidimensional data cloud. In
	this paper local principal curves are introduced, which are based
	on the localization of principal component analysis. The proposed
	algorithm is able to identify closed curves as well as multiple curves
	which may or may not be connected. For the evaluation of the performance
	of principal curves as tool for data reduction a measure of coverage
	is suggested. By use of simulated and real data sets the approach
	is compared to various alternative concepts of principal curves.},
  file = {:home/aepound/research/dimred/refs/localPrinCurves.pdf:pdf},
  owner = {aepound},
  publisher = {Springer},
  timestamp = {2012.02.07},
  opturl = {http://dx.doi.org/10.1007/s11222-005-4073-8}
}

@ARTICLE{Esser2011,
  author = {Esser, Ernie and M\"{o}ller, Michael and Osher, Stanley and Sapiro,
	Guillermo and Xin, Jack},
  title = {{A convex model for non-negative matrix factorization and dimensionality
	reduction on physical space}},
  journal = {ArXiv e-prints},
  year = {2011},
  volume = {Statistics:Machine Learning},
  pages = {14},
  month = feb,
  abstract = {A collaborative convex framework for factoring a data matrix \$X\$
	into a non-negative product \$AS\$, with a sparse coefficient matrix
	\$S\$, is proposed. We restrict the columns of the dictionary matrix
	\$A\$ to coincide with certain columns of the data matrix \$X\$,
	thereby guaranteeing a physically meaningful dictionary and dimensionality
	reduction. We use \$l\_\{1,\backslash infty\}\$ regularization to
	select the dictionary from the data and show this leads to an exact
	convex relaxation of \$l\_0\$ in the case of distinct noise free
	data. We also show how to relax the restriction-to-\$X\$ constraint
	by initializing an alternating minimization approach with the solution
	of the convex model, obtaining a dictionary close to but not necessarily
	in \$X\$. We focus on applications of the proposed framework to hyperspectral
	endmember and abundances identification and also show an application
	to blind source separation of NMR data.},
  arxivid = {1102.0844},
  file = {:home/aepound/research/overcomplete/hsi/2011-CvxModelNNMFdimReductPhysSpace-Esser-Moller.pdf:pdf},
  hsibands = {VNIR-SWIR (HyDICE)},
  hsinotes = {Uses HyDICE data to generate simulated data, and shows on one full
	data set.},
  keywords = {Machine Learning},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://arxiv.org/abs/1102.0844}
}

@ARTICLE{Fauvel2013, 
author={Fauvel, M. and Tarabalka, Y. and Benediktsson, J.A. and Chanussot, J. and Tilton, J.C.}, 
journal={Proceedings of the IEEE}, 
title={Advances in Spectral-Spatial Classification of Hyperspectral Images}, 
year={2013}, 
month=mar, 
volume={101}, 
number={3}, 
pages={652--675}, 
keywords={feature extraction;geophysical image processing;hyperspectral imaging;image classification;image resolution;image segmentation;mathematical morphology;support vector machines;vegetation mapping;conventional pixel level;homogeneous thematic maps;hyperspectral image segment;image morphological profile;mathematical morphology;morphological neighborhood;multiple-classifier system;object level;pixel-wise thematic map;presegmentation techniques;spanning forest algorithm;spatial information;spatial postprocessing;spatial structure contrast;spatial structure orientation;spatial structure size;spatially consistent thematic maps;spectral information;spectral-spatial classification;support vector machines;Classification algorithms;Feature extraction;Hyperspectral imaging;Image segmentation;Kernel;Nearest neighbor searches;Remote sensing;Spatial resolution;Spectral analysis;Classification;hyperspectral image;kernel methods;mathematical morphology;morphological neighborhood;segmentation;spectralâspatial classifier}, 
doi={10.1109/JPROC.2012.2197589}, 
ISSN={0018-9219},}

@article{Fischer2007,
author = {Fischer, Amber D. and Downes, T. V. and Leathers, R.},
title = {Median spectral-spatial bad pixel identification and replacement for 
hyperspectral SWIR sensors},
journal = {Proceedings of the SPIE},
volume = {6565},
number = {},
OPTpages = {65651E-65651E-12},
abstract = {Hyperspectral focal plane arrays typically contain many
pixels that are excessively noisy, dead, or exhibit poor signal to-
noise performance in comparison to the average pixel. These bad pixels
can significantly impair the performance of spectral target-detection
algorithms. Even a single missed bad pixel can lead to false
alarms. If the bad pixels are sparsely populated across the focal
plane, the over-sampling in both spatial and spectral dimensions of
the array can be capitalized upon to replace these pixels without
significant loss of information. However, bad pixels are frequently
localized in clusters, requiring a replacement strategy that rather
than providing a good estimate of the missing data will instead
minimize artifacts that may negatively affect the performance of
spectral detection algorithms. In this paper, we evaluate a robust
method to automatically identify bad pixels for short-wavelength
infrared (SWIR) hyperspectral sensors. In addition, we introduce a
novel procedure for the replacement of these pixels, which we
demonstrate provides a better estimate of the original pixel value
compared to interpolation methods for bad pixels found as both
isolated individuals and in clusters. The advantages of our technique
are discussed and demonstrated with data from several different
airborne sensor systems.}, 
year = {2007},
doi = {10.1117/12.720050},
optURL = { http://dx.doi.org/10.1117/12.720050},
eprint = {}
}

@INPROCEEDINGS{Freeman1997,
  author = {Freeman, J. and Downs, F. and Marcucci, L. and Lewis, E.N. and Blume,
	B. and Rish, J.},
  title = {Multispectral and hyperspectral imaging: applications for medical
	and surgical diagnostics},
  booktitle = {Engineering in Medicine and Biology Society, 1997. Proceedings of
	the 19th Annual International Conference of the IEEE},
  year = {1997},
  volume = {2},
  OPTpages = {700--701},
  OPTmonth = {oct-2 nov},
  __markedentry = {[aepound:6]},
  abstract = {One of the keys to a surgeon's successful work is his or her ability
	to see and feel well enough to adequately identify problems, particularly
	those that were not anticipated. Thus, an extension of the surgeon's
	vision would be a significant breakthrough. Multispectral and hyperspectral
	imaging techniques, along with associated algorithms and image processing
	methodologies have been developed by the military for detecting,
	classifying and identifying targets amid background clutter. Applying
	this technology to medicine will allow novel exploration of anatomy,
	physiology and pathology},
  doi = {10.1109/IEMBS.1997.757727},
  issn = {1094-687X},
  keywords = {algorithms;anatomy;background clutter;hyperspectral imaging;image
	processing methodologies;medical diagnostics;military;multispectral
	imaging;pathology;physiology;surgeon's vision extension;surgical
	diagnostics;target classification;target detection;target identification;infrared
	imaging;medical image processing;surgery;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@ARTICLE{Friedlander2008,
  author = {Friedlander, Michael P and Tseng, Paul},
  title = {{Exact Regularization of Convex Programs}},
  journal = {SIAM Journal on Optimization},
  year = {2008},
  volume = {18},
  pages = {1326},
  number = {4},
  abstract = {The regularization of a convex program is exact if all solutions of
	the regularized problem are also solutions of the original problem
	for all values of the regularization parameter below some positive
	threshold. For a general convex program, we show that the regularization
	is exact if and only if a certain selection problem has a Lagrange
	multiplier. Moreover, the regularization parameter threshold is inversely
	related to the Lagrange multiplier. We use this result to generalize
	an exact regularization result of Ferris and Mangasarian [Appl. Math.
	Optim., 23 (1991), pp. 266-273] involving a linearized selection
	problem. We also use it to derive necessary and sufficient conditions
	for exact penalization, similar to those obtained by Bertsekas [Math.
	Programming, 9 (1975), pp. 87-99] and by Bertsekas, Nedi\'{c}, and
	Ozdaglar [Convex Analysis and Optimization, Athena Scientific, Belmont,
	MA, 2003]. When the regularization is not exact, we derive error
	bounds on the distance from the regularized solution to the original
	solution set. We also show that existence of a âweak sharp minimumâ
	is in some sense close to being necessary for exact regularization.
	We illustrate the main result with numerical experiments on the \$\backslash
	ell\_1\$ regularization of benchmark (degenerate) linear programs
	and semidefinite/second-order cone programs. The experiments demonstrate
	the usefulness of \$\backslash ell\_1\$ regularization in finding
	sparse solutions.},
  doi = {10.1137/060675320},
  file = {:home/aepound/research/overcomplete/refs/toread/2007-ExactRegularizationCvxPrograms-Friedlander.pdf:pdf},
  issn = {10526234},
  keywords = {060675320,10,1137,49n15,65k10,90c05,90c25,90c51,ams subject classifications,conic
	program,convex program,degeneracy,doi,exact penalization,interior-point
	algorithms,lagrange multiplier,linear program,regularization,sparse
	solutions},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://link.aip.org/link/SJOPE8/v18/i4/p1326/s1\&Agg=doi}
}

@ARTICLE{Friedman1991,
  author = {Friedman, Jerome H.},
  title = {{Multivariate Adaptive Regression Splines}},
  journal = {The Annals of Statistics},
  year = {1991},
  volume = {19},
  pages = {1--67},
  number = {1},
  month = mar,
  abstract = {A new method is presented for flexible regression modeling of high
	dimensional data. The model takes the form of an expansion in product
	spline basis functions, where the number of basis functions as well
	as the parameters associated with each one (product degree and knot
	locations) are automatically determined by the data.$\backslash$par
	This procedure is motivated by the recursive partitioning approach
	to regression and shares its attractive properties. Unlike recursive
	partitioning, however, this method produces continuous models with
	continuous derivatives. It has more power and flexibility to model
	relationships that are nearly additive or involve interactions in
	at most a few variables. In addition, the model can be represented
	in a form that separately identifies the additive contributions and
	those associated with the different multivariable interactions.},
  doi = {10.1214/aos/1176347963},
  file = {:home/aepound/research/dimred/refs/euclid.aos.1176347963.pdf:pdf},
  issn = {0090-5364},
  keywords = {AID,CART,continuous derivatives,continuous models,expansion in product
	spline basis functions,high dimensional data,knot locations,multivariable
	function approximation,multivariate smoothing,nonparametric multiple
	regression,product degree,recursive partitioning approach to regression,statistical
	learning neural networks},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://projecteuclid.org/euclid.aos/1176347963}
}

@MISC{Friedman1995,
  author = {Friedman, J H and Roosen, C B},
  title = {{An introduction to multivariate adaptive regression splines}},
  year = {1995},
  booktitle = {Statistical Methods in Medical Research},
  number = {3},
  owner = {aepound},
  pages = {197},
  timestamp = {2012.02.07},
  opturl = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed\&cmd=Retrieve\&dopt=AbstractPlus\&list\_uids=9856040627074535162related:-o6YFLuwx4gJ},
  volume = {4}
}

@ARTICLE{Ghanem2011,
  author = {Bernard Ghanem and Narendra Ahuja},
  title = {A Probabilistic Framework for Discriminative Dictionary Learning},
  journal = {ArXiv e-prints},
  year = {2011},
  volume = {abs/1109.2389},
  pages = {1--10},
  month = sep,
  owner = {aepound},
  timestamp = {2012.02.16},
  opturl = {arXiv.org/abs/1109.2389}
}

@INPROCEEDINGS{Gillis2011,
  author = {Gillis, N. and Plemmons, R.J.},
  title = {Sparse nonnegative matrix underapproximation and its application
	to hyperspectral image analysis},
  booktitle = {3rd Workshop on Hyperspectral Image and Signal
  Processing: Evolution in Remote Sensing 
	(WHISPERS), 2011 },
  year = {2011},
  OPTpages = {1--4},
  month = jun,
  __markedentry = {[aepound:6]},
  abstract = {Dimensionality reduction techniques such as principal component analysis
	(PCA) are powerful tools for the analysis of high-dimensional data.
	In hyperspectral image analysis, nonnegativity of the data can be
	taken into account, leading to an additive linear model called nonnegative
	matrix factorization (NMF), which improves interpretability of the
	decomposition. Recently, another technique based on under-approximations
	(NMU) has been introduced, which allows the extraction of features
	in a recursive way, such as PCA, but preserving nonnegativity, such
	as NMF. However, for difficult hyperspectral datasets, even NMU can
	mix some materials together, and is therefore not able to separate
	of all materials properly for accurate target identification. In
	this paper we introduce sparse NMU by adding a sparsity constraint
	on the abundance matrix and use it to extract materials individually
	in a more efficient way than NMU. This is experimentally demonstrated
	on a HYDICE image of the San Diego airport.},
  doi = {10.1109/WHISPERS.2011.6080923},
  issn = {2158-6268},
  keywords = {HYDICE image;dimensionality reduction techniques;hyperspectral image
	analysis;nonnegative matrix factorization;principal component analysis;sparse
	nonnegative matrix underapproximation;under approximations;image
	classification;image processing;matrix decomposition;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@article{Goetz2009,
title = "Three decades of hyperspectral remote sensing of the Earth: A personal view ",
journal = "Remote Sensing of Environment ",
volume = "113, Supplement 1",
number = "0",
pages = {S5--S16},
year = "2009",
note = "Imaging Spectroscopy Special Issue ",
issn = "0034-4257",
doi = "http://dx.doi.org/10.1016/j.rse.2007.12.014",
opturl = "http://www.sciencedirect.com/science/article/pii/S003442570900073X",
author = "Alexander F.H. Goetz",
keywords = "Imaging spectrometry",
keywords = "Hyperspectral imaging",
keywords = "Spectroscopy",
keywords = "Earth observations",
keywords = "Remote sensing applications",
keywords = "Sensor development",
keywords = "Historical perspective "
}



@INPROCEEDINGS{Green2005,
  author = {Green, Sarah M and Cole, Jason A},
  title = {{Hyperspectral Remote Sensing and its Applications}},
  booktitle = {16th U.S. Department of Agriculture interagency research forum on
	gypsy moth and other invasive species 2005 edited Proceedings},
  year = {2005},
  editor = {Gottschalk, Kurt W.},
  pages = {17325--17325},
  address = {Annapolis, MD},
  publisher = {U.S. Forest Service},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Green, Cole - 2005 - Hyperspectral Remote Sensing and its Applications.pdf:pdf},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://www.treesearch.fs.fed.us/pubs/20862}
}

@ARTICLE{Greer2011,
  author = {Greer, J},
  title = {{Sparse demixing of HyperSpectral Images}},
  journal = {IEEE transactions on image processing},
  year = {2011},
  volume = {21},
  pages = {219--228},
  number = {1},
  month = jun,
  abstract = {In the linear mixture model for hyperspectral images, all the image
	spectra lie on a high-dimensional simplex with corners called endmembers.
	Given a set of endmembers, the standard calculation of fractional
	abundances with constrained least squares typically identifies the
	spectra as combinations of most, if not all, endmembers. We instead
	assume that pixels are combinations of only a few endmembers, yielding
	abundance vectors that are sparse. We introduce Sparse Demixing (SD),
	a method, similar to Orthogonal Matching Pursuit (OMP), for calculating
	these sparse abundances. We demonstrate that SD outperforms an existing
	L1 demixing algorithm, which we prove depends adversely on the angles
	between endmembers. We combine SD with dictionary learning methods
	to automatically calculate endmembers for a provided set of spectra.
	Applying to an AVIRIS image of Cuprite, Nevada yields endmembers
	that compare favorably with signatures from the USGS spectral library.},
  doi = {10.1109/TIP.2011.2160189},
  file = {:home/aepound/research/overcomplete/hsi/2011-SparseDemixingHSI-Greer.pdf:pdf},
  issn = {1941-0042},
  keywords = {LMM;SD;USGS spectral library;airborne visible-infrared imaging spectrometer;dictionary
	learning method;high-dimensional simplex;hyperspectral image;orthogonal
	matching pursuit;sparse demixing;geophysical image processing;infrared
	spectrometers;iterative methods;least squares approximations;time-frequency
	analysis;vectors;},
  owner = {aepound},
  pmid = {21693418},
  timestamp = {2012.02.07},
  opturl = {http://www.ncbi.nlm.nih.gov/pubmed/21693418}
}

@ARTICLE{Guigue2006,
  author = {Guigue, Vincent and Rakotomamonjy, Alain and Canu, St\'{e}phane},
  title = {{Kernel Basis Pursuit}},
  journal = {Revue d'intelligence artificielle},
  year = {2006},
  volume = {20},
  pages = {757--774},
  number = {6},
  month = dec,
  doi = {10.3166/ria.20.757-774},
  file = {:home/aepound/research/overcomplete/refs/toread/2005-KernelBasisPursuit-Guigue.pdf:pdf},
  issn = {0992499X},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ria.revuesonline.com/article.jsp?articleId=8934}
}

@INPROCEEDINGS{Guigue2005,
  author = {Vincent Guigue and Alain Rakotomamonjy and StÃ©phane Canu},
  title = {S.: Kernel Basis Pursuit},
  booktitle = {Proceedings of the 16th European Conference on Machine Learning},
  year = {2005},
  publisher = {Springer},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@INPROCEEDINGS{Guo2009,
  author = {Guo, Zhaohui and Wittman, Todd and Osher, Stanley},
  title = {{L1 unmixing and its application to hyperspectral image enhancement}},
  booktitle = {Proceedings of SPIE},
  year = {2009},
  pages = {73341M--73341M--9},
  publisher = {SPIE},
  doi = {10.1117/12.818245},
  file = {:home/aepound/research/overcomplete/hsi/2009-L1unmixingAppsHSIenhancement-Guo.pdf:pdf},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://link.aip.org/link/PSISDG/v7334/i1/p73341M/s1\&Agg=doi}
}

@ARTICLE{Ham2005,
  author = {Ham, J. and Yangchi Chen and Crawford, M.M. and Ghosh, J.},
  title = {Investigation of the random forest framework for classification of
	hyperspectral data},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year = {2005},
  volume = {43},
  pages = {492--501},
  number = {3},
  month = mar,
  __markedentry = {[aepound:6]},
  abstract = { Statistical classification of byperspectral data is challenging because
	the inputs are high in dimension and represent multiple classes that
	are sometimes quite mixed, while the amount and quality of ground
	truth in the form of labeled data is typically limited. The resulting
	classifiers are often unstable and have poor generalization. This
	work investigates two approaches based on the concept of random forests
	of classifiers implemented within a binary hierarchical multiclassifier
	system, with the goal of achieving improved generalization of the
	classifier in analysis of hyperspectral data, particularly when the
	quantity of training data is limited. A new classifier is proposed
	that incorporates bagging of training samples and adaptive random
	subspace feature selection within a binary hierarchical classifier
	(BHC), such that the number of features that is selected at each
	node of the tree is dependent on the quantity of associated training
	data. Results are compared to a random forest implementation based
	on the framework of classification and regression trees. For both
	methods, classification results obtained from experiments on data
	acquired by the National Aeronautics and Space Administration (NASA)
	Airborne Visible/Infrared Imaging Spectrometer instrument over the
	Kennedy Space Center, Florida, and by Hyperion on the NASA Earth
	Observing 1 satellite over the Okavango Delta of Botswana are superior
	to those from the original best basis BHC algorithm and a random
	subspace extension of the BHC.},
  doi = {10.1109/TGRS.2004.842481},
  issn = {0196-2892},
  keywords = { Airborne Visible/Infrared Imaging Spectrometer; Botswana; Florida;
	Hyperion; Kennedy Space Center; NASA Earth Observing 1 satellite;
	Okavango Delta; USA; adaptive random subspace feature selection;
	binary hierarchical classifier; binary hierarchical multiclassifier
	system; classification tree; hyperspectral data analysis; hyperspectral
	data classification; random forest; random subspace extension; regression
	tree; statistical classification; tree node; data acquisition; feature
	extraction; forestry; geophysical signal processing; image classification;
	multidimensional signal processing; random processes; trees (mathematics);
	vegetation mapping;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@BOOK{Han2007,
  title = {{Investigation of nonlinearity in hyperspectral remotely sensed imagery
	â a nonlinear time series analysis approach}},
  publisher = {IEEE},
  year = {2007},
  author = {Han, Tian and Goodenough, David G.},
  pages = {1556--1560},
  booktitle = {2007 IEEE International Geoscience and Remote Sensing Symposium},
  doi = {10.1109/IGARSS.2007.4423107},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Goodenough - 2007 - Investigation of nonlinearity in hyperspectral remotely sensed imagery â a nonlinear time series analysis approach.pdf:pdf},
  isbn = {978-1-4244-1211-2},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4423107}
}

@Book{htf2009,
  author =       {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  title =        {The Elements of Statistical Learning},
  publisher =    {Springer},
  year =         2009,
  OPTeditor =    "",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTaddress =   "",
  OPTedition =   "",
  OPTmonth =     "",
  OPTnote =      "",
  OPTannote =    ""
}

@PHDTHESIS{Hastie1984,
  author = {Hastie, T},
  title = {{Principal Curves and Surfaces}},
  school = {Stanford University},
  year = {1984},
  booktitle = {Work},
  file = {:home/aepound/research/dimred/refs/slac-r-276.pdf:pdf},
  number = {November},
  owner = {aepound},
  timestamp = {2012.02.07},
  volume = {32}
}

@ARTICLE{Hastie2007,
  author = {Hastie, Trevor and Taylor, Jonathan and Tibshirani, Robert and Walther,
	Guenther},
  title = {{Forward stagewise regression and the monotone lasso}},
  journal = {Electronic Journal of Statistics},
  year = {2007},
  volume = {1},
  pages = {1--29},
  doi = {10.1214/07-EJS004},
  file = {:home/aepound/research/overcomplete/refs/toread/2006-ForwardStagewiseRegression-Hastie.pdf:pdf},
  issn = {1935-7524},
  keywords = {62j99,94305,ams classification,and health,ca,depts,edu,hastie,lasso,of
	statistics,policy,regression,research,sequoia hall,stagewise,stanford,stanford
	univ,stat},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://projecteuclid.org/euclid.ejs/1177687773}
}

@BOOK{ElemStatLearn2008,
  title = {The elements of statistical learning: data mining, inference, and
	prediction},
  publisher = {New York: Springer-Verlag},
  year = {2008},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  pages = {763},
  edition = {2nd},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@MISC{Huang2006,
  author = {Huang, Jian and Ma, Shuangge and Zhang, Cun-hui},
  title = {{Adaptive Lasso for Sparse High-dimensional Regression}},
  year = {2006},
  abstract = {We study the asymptotic properties of adaptive LASSO estimators in
	sparse, high-dimensional, linear regression models when the number
	of covariates may increase with the sample size. We consider variable
	selection using the adaptive LASSO, where the L1 norms in the penalty
	are re-weighted by data-dependent weights. We show that, if a reasonable
	initial estimator is available, then under appropriate conditions,
	adaptive LASSO correctly select covariates with nonzero coefficients
	with probability converging to one and that the estimators of nonzero
	coefficients have the same asymptotic dis-tribution that they would
	have if the zero coefficients were known in advance. Thus, the adaptive
	LASSO has an oracle property in the sense of Fan and Li (2001) and
	Fan and Peng (2004). In addition, under a partial orthogonality condition
	in which the covariates with zero coefficients are weakly correlated
	with the covariates with nonzero coefficients, univariate regression
	can be used to obtain the initial estimator. With this initial estimator,
	adaptive LASSO has the oracle property even when the number of covariates
	is greater than the sample size.},
  booktitle = {ReVision},
  file = {:home/aepound/research/overcomplete/refs/toread/2006-AdapLassoHiDimRegress-Huang.pdf:pdf},
  institution = {University of Iowa},
  keywords = {Penalized regression,asymptotic normality,high-dimensional data,oracle
	property,variable selection,zero-consistency},
  number = {374},
  owner = {aepound},
  pages = {1--28},
  timestamp = {2012.02.07},
  opturl = {http://www.stat.uiowa.edu/techrep/tr374-rev1.pdf },
  volume = {1}
}

@INPROCEEDINGS{Huang2009,
  author = {Huang, Junzhou and Zhang, Tong and Metaxas, Dimitris},
  title = {{Learning with structured sparsity}},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine
	Learning},
  year = {2009},
  series = {ICML '09},
  pages = {417--424},
  address = {New York, New York, USA},
  publisher = {ACM Press},
  doi = {10.1145/1553374.1553429},
  file = {:home/aepound/research/overcomplete/refs/toread/2009-LearningWithStructuredSparsity-Huang.pdf:pdf},
  isbn = {9781605585161},
  journal = {Proceedings of the 26th Annual International Conference on Machine
	Learning - ICML '09},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://portal.acm.org/citation.cfm?doid=1553374.1553429}
}

@ARTICLE{Iordache2011,
  author = {Iordache, Marian-daniel and Bioucas-Dias, Jos\'{e} M. and Plaza,
	Antonio},
  title = {Sparse Unmixing of Hyperspectral Data},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year = {2011},
  volume = {49},
  pages = {2014--2039},
  number = {6},
  month = jun,
  abstract = {Linear spectral unmixing is a popular tool in remotely sensed hyperspectral
	data interpretation. It aims at estimating the fractional abundances
	of pure spectral signatures (also called as endmembers) in each mixed
	pixel collected by an imaging spectrometer. In many situations, the
	identification of the end-member signatures in the original data
	set may be challenging due to insufficient spatial resolution, mixtures
	happening at different scales, and unavailability of completely pure
	spectral signatures in the scene. However, the unmixing problem can
	also be approached in semisupervised fashion, i.e., by assuming that
	the observed image signatures can be expressed in the form of linear
	combinations of a number of pure spectral signatures known in advance
	(e.g., spectra collected on the ground by a field spectroradiometer).
	Unmixing then amounts to finding the optimal subset of signatures
	in a (potentially very large) spectral library that can best model
	each mixed pixel in the scene. In practice, this is a combinatorial
	problem which calls for efficient linear sparse regression (SR) techniques
	based on sparsity-inducing regularizers, since the number of endmembers
	participating in a mixed pixel is usually very small compared with
	the (ever-growing) dimensionality (and availability) of spectral
	libraries. Linear SR is an area of very active research, with strong
	links to compressed sensing, basis pursuit (BP), BP denoising, and
	matching pursuit. In this paper, we study the linear spectral unmixing
	problem under the light of recent theoretical results published in
	those referred to areas. Furthermore, we provide a comparison of
	several available and new linear SR algorithms, with the ultimate
	goal of analyzing their potential in solving the spectral unmixing
	problem by resorting to available spectral libraries. Our experimental
	results, conducted using both simulated and real hyperspectral data
	sets collected by the NASA Jet Propulsion Laboratory's Airborne Visible
	I- - nfrared Imaging Spectrometer and spectral libraries publicly
	available from the U.S. Geological Survey, indicate the potential
	of SR techniques in the task of accurately characterizing the mixed
	pixels using the library spectra. This opens new perspectives for
	spectral unmixing, since the abundance estimation process no longer
	depends on the availability of pure spectral signatures in the input
	data nor on the capacity of a certain endmember extraction algorithm
	to identify such pure signatures.},
  doi = {10.1109/TGRS.2010.2098413},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iordache, Bioucas-Dias, Plaza - 2011 - Sparse Unmixing of Hyperspectral Data.pdf:pdf},
  issn = {0196-2892},
  keywords = {Coherence,Hyperspectral imaging,Libraries,Materials,Pixel,Strontium},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5692827}
}

@INPROCEEDINGS{Jiang2011,
  author = {Zhuolin Jiang and Zhe Lin and Davis, L.S.},
  title = {{Learning a discriminative dictionary for sparse coding via label
	consistent K-SVD}},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference
	on},
  year = {2011},
  pages = {1697--1704},
  month = jun,
  abstract = {A label consistent K-SVD (LC-KSVD) algorithm to learn a discriminative
	dictionary for sparse coding is presented. In addition to using class
	labels of training data, we also associate label information with
	each dictionary item (columns of the dictionary matrix) to enforce
	discriminability in sparse codes during the dictionary learning process.
	More specifically, we introduce a new label consistent constraint
	called `discriminative sparse-code error' and combine it with the
	reconstruction error and the classification error to form a unified
	objective function. The optimal solution is efficiently obtained
	using the K-SVD algorithm. Our algorithm learns a single over-complete
	dictionary and an optimal linear classifier jointly. It yields dictionaries
	so that feature points with the same class labels have similar sparse
	codes. Experimental results demonstrate that our algorithm outperforms
	many recently proposed sparse coding techniques for face and object
	category recognition under the same learning conditions.},
  doi = {10.1109/CVPR.2011.5995354},
  issn = {1063-6919},
  keywords = {K-SVD;classification error;dictionary learning process;discriminative
	sparse code error;face recognition;label consistent;object category
	recognition;optimal linear classifier;reconstruction error;training
	data;dictionaries;face recognition;image classification;image coding;learning
	(artificial intelligence);object recognition;singular value decomposition;},
  owner = {aepound},
  timestamp = {2012.02.13}
}

@INPROCEEDINGS{Joelsson2005,
  author = {Joelsson, S.R. and Benediktsson, J.A. and Sveinsson, J.R.},
  title = {Random forest classifiers for hyperspectral data},
  booktitle = {2005 IEEE International Geoscience and Remote Sensing
  Symposium Proceedings}, 
  year = {2005},
  volume = {1},
  OPTpages = { 4 pp.},
  month = jul,
  __markedentry = {[aepound:6]},
  abstract = { Two random forest (RF) approaches are explored; the RF-BHC (binary
	hierarchical classifier) and the RF-CART (classification and regression
	tree). Both methods are based on a collection (forest) of tree-like
	classifier systems where the difference is in the way the trees are
	grown. The BHC approach depends on class separability measures and
	the Fisher projection, which maximizes the Fisher discriminant where
	each tree is a class hierarchy, and the number of leaves is the same
	as the number of classes. The CART approach is based on CART-like
	trees where trees are grown to minimize an impurity measure. Here,
	these different RF approaches are compared in experiments. The RF
	approaches were investigated in experiments by classification of
	an urban area from Pavia, Italy using hyperspectral ROSIS (reflective
	optics system imaging spectrometer) data provided by DLR.},
  doi = {10.1109/IGARSS.2005.1526129},
  keywords = { Fisher discriminant; Fisher projection; Italy; Pavia; binary hierarchical
	classifier; class separability measure; classification tree; hyperspectral
	ROSIS data; hyperspectral data; impurity measure; random forest classifiers;
	reflective optics system imaging spectrometer; regression tree; geophysical
	signal processing; geophysical techniques; image classification;
	multidimensional signal processing; regression analysis; remote sensing;
	signal classification; spectral analysis; trees (mathematics);},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@ARTICLE{Kegl2000,
  author = {Kegl, B and Krzyzak, A and Linder, T and Zeger, K},
  title = {{Learning and design of principal curves}},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2000},
  volume = {22},
  pages = {281--297},
  number = {3},
  month = mar,
  abstract = {Principal curves have been defined as ldquo;self-consistent rdquo;
	smooth curves which pass through the ldquo;middle rdquo; of a d-dimensional
	probability distribution or data cloud. They give a summary of the
	data and also serve as an efficient feature extraction tool. We take
	a new approach by defining principal curves as continuous curves
	of a given length which minimize the expected squared distance between
	the curve and points of the space randomly chosen according to a
	given distribution. The new definition makes it possible to theoretically
	analyze principal curve learning from training data and it also leads
	to a new practical construction. Our theoretical learning scheme
	chooses a curve from a class of polygonal lines with k segments and
	with a given total length to minimize the average squared distance
	over n training points drawn independently. Convergence properties
	of this learning scheme are analyzed and a practical version of this
	theoretical algorithm is implemented. In each iteration of the algorithm,
	a new vertex is added to the polygonal line and the positions of
	the vertices are updated so that they minimize a penalized squared
	distance criterion. Simulation results demonstrate that the new algorithm
	compares favorably with previous methods, both in terms of performance
	and computational complexity, and is more robust to varying data
	models},
  doi = {10.1109/34.841759},
  file = {:home/aepound/research/dimred/refs/learnDesignPrinCurves.pdf:pdf},
  issn = {01628828},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=841759}
}

@article{Kieffer1996,
author = {Kieffer, Hugh H.},
title = {Detection and correction of bad pixels in hyperspectral sensors},
journal = {Proceedings of the SPIE},
volume = {2821},
number = {},
pages = {93--108},
abstract = {Hyperspectral sensors may use a 2D array such that one direction across the array is spatial and the other direction is spectral. Any pixels therein having very poor signal-to-noise performance must have their values replaced. Because of the anisotropic nature of information at the array, common image processing techniques should not be used. A bad-pixel replacement algorithm has been developed which uses the information closest in both spectral and spatial sense to obtain a value which has both the spectral and reflectance properties of the adjacent terrain in the image. A simple and fast implementation that `repairs' individual bad pixels or clusters of bad pixels has three steps; the first two steps are done only once: (1) Pixels are flagged as `bad' if their noise level or responsivity fall outside acceptable limits for their spectral channel. (2) For each bad pixel, the minimum-sized surrounding rectangle is determined that has good pixels at all 4 corners and at the 4 edge-points where the row/column of the bad pixel intersect the rectangle boundary (five cases are possible due to bad pixels near an edge or corner of the detector array); the specifications of this rectangle are saved. (3) After a detector data frame has been radiometrically corrected (dark subtraction and gain corrections), the spectral shapes represented by the rectangle edges extending in the dispersion direction are averaged; this shape is then interpolated through the two pixels in the other edges of the rectangle. This algorithm has been implemented for HYDICE.},
year = {1996},
doi = {10.1117/12.257162},
optURL = { http://dx.doi.org/10.1117/12.257162},
eprint = {}
}

@ARTICLE{KreutzDelgado2003,
  author = {Kreutz-Delgado, Kenneth and Murray, Joseph F and Rao, Bhaskar D and
	Engan, Kjersti and Lee, Te-Won and Sejnowski, Terrence J},
  title = {{Dictionary learning algorithms for sparse representation.}},
  journal = {Neural computation},
  year = {2003},
  volume = {15},
  pages = {349--96},
  number = {2},
  month = feb,
  abstract = {Algorithms for data-driven learning of domain-specific overcomplete
	dictionaries are developed to obtain maximum likelihood and maximum
	a posteriori dictionary estimates based on the use of Bayesian models
	with concave/Schur-concave (CSC) negative log priors. Such priors
	are appropriate for obtaining sparse representations of environmental
	signals within an appropriately chosen (environmentally matched)
	dictionary. The elements of the dictionary can be interpreted as
	concepts, features, or words capable of succinct expression of events
	encountered in the environment (the source of the measured signals).
	This is a generalization of vector quantization in that one is interested
	in a description involving a few dictionary entries (the proverbial
	"25 words or less"), but not necessarily as succinct as one entry.
	To learn an environmentally adapted dictionary capable of concise
	expression of signals generated by the environment, we develop algorithms
	that iterate between a representative set of sparse representations
	found by variants of FOCUSS and an update of the dictionary using
	these sparse representations. Experiments were performed using synthetic
	data and natural images. For complete dictionaries, we demonstrate
	that our algorithms have improved performance over other independent
	component analysis (ICA) methods, measured in terms of signal-to-noise
	ratios of separated sources. In the overcomplete case, we show that
	the true underlying dictionary and sparse sources can be accurately
	recovered. In tests with natural images, learned overcomplete dictionaries
	are shown to have higher coding efficiency than complete dictionaries;
	that is, images encoded with an overcomplete dictionary have both
	higher compression (fewer bits per pixel) and higher accuracy (lower
	mean square error).},
  doi = {10.1162/089976603762552951},
  file = {:home/aepound/research/overcomplete/refs/2003-DictionaryLearningAlgsSparseRep-Kreutz-Delgado.pdf:pdf},
  issn = {0899-7667},
  keywords = {Algorithms,Artificial Intelligence,Learning,Learning: physiology,Stochastic
	Processes},
  owner = {aepound},
  pmid = {12590811},
  timestamp = {2012.02.07},
  opturl = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2944020\&tool=pmcentrez\&rendertype=abstract}
}

@INPROCEEDINGS{BorChen2008,
  author = {Bor-Chen Kuo and Jinn-Min Yang and Tian-Wei Sheu and Szu-Wei Yang},
  title = {Kernel-Based KNN and Gaussian Classifiers for Hyperspectral Image
	Classification},
  booktitle = {IEEE
	International Geoscience and Remote Sensing Symposium},
  year = {2008},
  volume = {2},
  OPTpages = {II-1006--II-1008},
  month = jul,
  __markedentry = {[aepound:6]},
  abstract = {In this study, two kernel-based classifiers are applied to hyperspectral
	image classification. One is the kernel Gaussian classifier, and
	the other is the kernel k-nearest-neighbor classifier. For classification
	in feature space, the data are mapped from the input-space into a
	higher dimensional feature space by utilizing a nonlinear transformation,
	and then we can perform the k-nearest-neighbor classifier and the
	Gaussian classifier on the mapped images in that space. Fortunately,
	instead of doing the expensive transformation of samples, the classification
	can be performed via inner products in feature space and use the
	kernel function to efficiently compute the inner products which is
	the so-called kernel trick. The effectiveness of the proposed classifiers
	is evaluated by real datasets and other classifiers are included
	for comparison. The experimental results show that the kernel Gaussian
	classifier outperforms the others.},
  doi = {10.1109/IGARSS.2008.4779167},
  keywords = {feature space;hyperspectral image classification;kernel Gaussian classifier;kernel
	k-nearest-neighbor classifier;kernel trick;geophysical signal processing;geophysical
	techniques;image classification;remote sensing;support vector machines;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@INPROCEEDINGS{Leathers2006, 
author={Leathers, R. and Downes, T.V.}, 
booktitle={IEEE International Conference on Geoscience and Remote
Sensing Symposium},  
title={Scene-based Nonuniformity Correction and Bad-pixel
Identification for Hyperspectral VNIR/SWIR Sensors},  
year={2006}, 
month=jul, 
OPTpages={2373--2376}, 
keywords={Calibration;Cameras;Hyperspectral imaging;Hyperspectral
sensors;Infrared sensors;Laboratories;Layout;Optical computing;Optical
sensors;Radiometry},  
doi={10.1109/IGARSS.2006.614},
}

@ARTICLE{LeBlanc1994,
  author = {LeBlanc, Michael and Tibshirani, Robert},
  title = {{Adaptive Principal Surfaces}},
  journal = {Journal of the American Statistical Association},
  year = {1994},
  volume = {89},
  pages = {53--64},
  number = {425},
  file = {:home/aepound/research/dimred/refs/adapPrinSurf.pdf:pdf},
  issn = {01621459},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@INPROCEEDINGS{Lee2007,
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y},
  title = {{Efficient sparse coding algorithms}},
  booktitle = {In NIPS},
  year = {2007},
  pages = {801--808},
  abstract = {Sparse coding provides a class of algorithms for finding succinct
	representations of stimuli; given only unlabeled input data, it discovers
	basis functions that capture higher-level features in the data. However,
	finding sparse codes remains a very difficult computational problem.
	In this paper, we present efficient sparse coding algorithms that
	are based on iteratively solving two convex optimization problems:
	an L1-regularized least squares problem and an L2-constrained least
	squares problem. We propose novel algorithms to solve both of these
	optimization problems. Our algorithms result in a significant speedup
	for sparse coding, allowing us to learn larger sparse codes than
	possible with previously described algorithms. We apply these algorithms
	to natural images and demonstrate that the inferred sparse codes
	exhibit end-stopping and non-classical receptive field surround suppression
	and, therefore, may provide a partial explanation for these two phenomena
	in V1 neurons.},
  doi = {10.1.1.69.2112},
  file = {:home/aepound/research/overcomplete/refs/2007-EfficientSparseCodingAlgs-Lee.pdf:pdf},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@INPROCEEDINGS{Lennon2002,
  author = {Lennon, M. and Mercier, G. and Hubert-Moy, L.},
  title = {Classification of hyperspectral images with nonlinear filtering and
	support vector machines},
  booktitle = {2002 IEEE
	International Geoscience and Remote Sensing Symposium},
  year = {2002},
  volume = {3},
  OPTpages = {1670--1672},
  month = jun,
  __markedentry = {[aepound:6]},
  abstract = { Support vector machines, recently introduced in hyperspectral imagery,
	are applied to classify land cover on images from the airborne CASI
	sensor with a small training set. A smoothing preprocessing step
	is achieved, based on a vectorial extension of the anisotropic diffusion
	nonlinear filtering process. It allows the separability of the classes
	to be increased as well as homogeneous areas to be smoothed. It comes
	to take into consideration the spatial context before the classification,
	leading to improve the classification rate and to produce noiselessly
	classification maps with support vector machines.},
  doi = {10.1109/IGARSS.2002.1026216},
  keywords = { 400 to 2500 nm; CASI; IR; airborne sensor; anisotropic diffusion
	nonlinear filtering; geophysical measurement technique; geophysics
	computing; hyperspectral remote sensing; image classification; infrared;
	land cover; land surface; multidimensional signal processing; multispectral
	remote sensing; nonlinear filtering; separability; small training
	set; smoothing preprocessing step; support vector machine; terrain
	mapping; vectorial extension; vegetation mapping; visible; geophysical
	signal processing; geophysical techniques; image classification;
	learning automata; multidimensional signal processing; remote sensing;
	terrain mapping; vegetation mapping;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@ARTICLE{Lewicki2000,
  author = {Lewicki, M S and Sejnowski, T J},
  title = {{Learning overcomplete representations.}},
  journal = {Neural computation},
  year = {2000},
  volume = {12},
  pages = {337--65},
  number = {2},
  month = feb,
  abstract = {In an overcomplete basis, the number of basis vectors is greater than
	the dimensionality of the input, and the representation of an input
	is not a unique combination of basis vectors. Overcomplete representations
	have been advocated because they have greater robustness in the presence
	of noise, can be sparser, and can have greater flexibility in matching
	structure in the data. Overcomplete codes have also been proposed
	as a model of some of the response properties of neurons in primary
	visual cortex. Previous work has focused on finding the best representation
	of a signal using a fixed overcomplete basis (or dictionary). We
	present an algorithm for learning an overcomplete basis by viewing
	it as probabilistic model of the observed data. We show that overcomplete
	bases can yield a better approximation of the underlying statistical
	distribution of the data and can thus lead to greater coding efficiency.
	This can be viewed as a generalization of the technique of independent
	component analysis and provides a method for Bayesian reconstruction
	of signals in the presence of noise and for blind source separation
	when there are more sources than mixtures.},
  file = {:home/aepound/research/overcomplete/refs/2000-OvercompleteRep-Lewicki-Sejnowski.pdf:pdf},
  issn = {0899-7667},
  keywords = {Algorithms,Animals,Humans,Learning,Likelihood Functions,Models, Neurological,Models,
	Statistical,Probability,Speech},
  owner = {aepound},
  pmid = {10636946},
  timestamp = {2012.02.07},
  opturl = {http://www.ncbi.nlm.nih.gov/pubmed/10636946}
}

@article{Li2012,
  title={Blind nonlinear hyperspectral unmixing based on constrained
    kernel nonnegative matrix factorization}, 
  author={Li, Xiaorun and Cui, Jiantao and Zhao, Liaoying},
  issn={1863-1703},
  journal={Signal, Image and Video Processing},
  doi={10.1007/s11760-012-0392-3},
  opturl={http://dx.doi.org/10.1007/s11760-012-0392-3},
  publisher={Springer-Verlag},
  keywords={Nonlinear mixture model; Hyperspectral imagery;
    Nonnegative matrix factorization (NMF); Kernel function; Spectral
    unmixing}, 
  pages={1--13},
  year={2012},
  language={English}
} 
				  
@ARTICLE{Liu2012,
  author = {Junmin Liu and Jiangshe Zhang},
  title = {A New Maximum Simplex Volume Method Based on Householder Transformation
	for Endmember Extraction},
  journal = {Geoscience and Remote Sensing, IEEE Transactions on},
  year = {2012},
  volume = {50},
  pages = {104--118},
  number = {1},
  month = jan,
  __markedentry = {[aepound:6]},
  abstract = {Endmember extraction is very important in hyperspectral image analysis.
	The accurate identification of endmembers enables target detection
	and classification and efficient spectral unmixing. Although a number
	of endmember extraction algorithms have been proposed, such as two
	state-of-the-art algorithms-vertex component analysis (VCA) and simplex
	growing algorithm (SGA)-it is still a rather challenging task. In
	this paper, a new maximum simplex volume method based on Householder
	transformation (HT), referred to as maximum volume by HT (MVHT),
	is presented for endmember extraction. The proposed algorithm provides
	consistent results with low computational complexity, which overcomes
	the disadvantage of the inconsistent result of VCA and the shortcoming
	of the high computational cost of SGA resulted from calculating the
	simplex volume. A comparative study and analysis are conducted among
	the three endmember extraction algorithms, VCA, SGA, and MVHT, on
	both simulated and real hyperspectral data. The obtained experimental
	results demonstrate that the proposed MVHT algorithm generally provides
	a competitive or even better performance over VCA and SGA.},
  doi = {10.1109/TGRS.2011.2158829},
  issn = {0196-2892},
  keywords = {MVHT algorithm;computational complexity;endmember extraction algorithm;householder
	transformation;hyperspectral data;hyperspectral image analysis;maximum
	simplex volume method;remote sensing image processing;simplex growing
	algorithm;spectral unmixing;state-of-the-art algorithms;target detection;vertex
	component analysis;geophysical image processing;remote sensing;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@MISC{Moller2010,
  author = {M\"{o}ller, Michael and Esser, Ernie and Osher, Stanley and Sapiro,
	Guillermo and Xin, Jack},
  title = {A Convex Model for Matrix Factorization and Dimensionality Reduction
	on Physical Space and its Application to Blind Hyperspectral Unmixing},
  year = {2010},
  abstract = {A collaborative convex framework for factoring a data matrix X into
	a non-negative product AS, with a sparse coefficient matrix S, is
	introduced. We restrict the columns of the dictionary matrix A to
	coincide with certain columns of X, thereby guaranteeing a physically
	meaningful dictionary and dimensionality reduction. As an example,
	we show applications of the proposed framework on hyperspectral endmember
	and abundances identification.},
  booktitle = {Evaluation},
  file = {:home/aepound/research/overcomplete/hsi/2010-ConvexNNMFforDimReduct\_BlindHSI-Moller-etal.pdf:pdf},
  institution = {University of Minnesota},
  keywords = {coefficients,collaborative techniques,convex sets,factor analysis,hyperspectral
	imagery,identification,mixingl matrix theory,models,sparse matrix},
  owner = {aepound},
  pages = {1--6},
  timestamp = {2012.02.07},
  opturl = {http://oai.dtic.mil/oai/oai?verb=getRecord\&metadataPrefix=html\&identifier=ADA540658}
}

@INPROCEEDINGS{Mairal2009c,
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  title = {{Online dictionary learning for sparse coding}},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine
	Learning - ICML '09},
  year = {2009},
  pages = {1--8},
  address = {New York, New York, USA},
  publisher = {ACM Press},
  doi = {10.1145/1553374.1553463},
  file = {:home/aepound/research/overcomplete/refs/2009-OnlineDictLearnSparseCoding-Mairal.pdf:pdf},
  isbn = {9781605585161},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://portal.acm.org/citation.cfm?doid=1553374.1553463}
}

@ARTICLE{Mairal2009j,
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  title = {Online Learning for Matrix Factorization and Sparse Coding},
  journal = {The Journal of Machine Learning Research},
  year = {2009},
  volume = {11},
  pages = {1--43},
  month = aug,
  abstract = {Sparse coding--that is, modelling data vectors as sparse linear combinations
	of basis elements--is widely used in machine learning, neuroscience,
	signal processing, and statistics. This paper focuses on the large-scale
	matrix factorization problem that consists of learning the basis
	set, adapting it to specific data. Variations of this problem include
	dictionary learning in signal processing, non-negative matrix factorization
	and sparse principal component analysis. In this paper, we propose
	to address these tasks with a new online optimization algorithm,
	based on stochastic approximations, which scales up gracefully to
	large datasets with millions of training samples, and extends naturally
	to various matrix factorization formulations, making it suitable
	for a wide range of learning problems. A proof of convergence is
	presented, along with experiments with natural images and genomic
	data demonstrating that it leads to state-of-the-art performance
	in terms of speed and optimization for both small and large datasets.},
  arxivid = {0908.0050},
  file = {:home/aepound/research/overcomplete/refs/toread/2009-OnlineLearningMatFactSparseCoding-Mairal.pdf:pdf},
  keywords = {Learning,Machine Learning,Optimization and Control},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://arxiv.org/abs/0908.0050}
}

@MISC{Mairal2008,
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo
	and Zisserman, Andrew},
  title = {Supervised Dictionary Learning},
  year = {2008},
  arxivid = {arXiv:0809.3083v1},
  booktitle = {Infoatique/Vision par ordinateur et reconnaissance de formes},
  file = {:home/aepound/research/overcomplete/refs/2008-SupervisedDictLearning-Mairal-et-al.pdf:pdf},
  keywords = {classification,dictionary learning,sparsoty},
  number = {September},
  owner = {aepound},
  pages = {15},
  timestamp = {2012.02.07},
  opturl = {http://hal.inria.fr/inria-00322431/fr/}
}

@ARTICLE{Mairal2010,
   author = {{Mairal}, J. and {Bach}, F. and {Ponce}, J.},
    title = "Task-Driven Dictionary Learning",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1009.5358},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning},
     year = 2010,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1009.5358M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Mairal2012, 
author={Mairal, J. and Bach, F. and Ponce, J.}, 
journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on}, 
title={Task-Driven Dictionary Learning}, 
year={2012}, 
month=apr, 
volume={34}, 
number={4}, 
pages={791--804}, 
keywords={compressed sensing;data models;handwritten character recognition; 
image classification;image representation;image restoration;
learning (artificial intelligence); matrix decomposition;regression analysis;
classical optimization tools;compressed sensing; data modeling;
digital art identification;handwritten digit classification;
image classification; large-scale matrix factorization problem;
learned dictionary;linear combinations;machine learning; natural images;
neuroscience;nonlinear inverse image problems;regression tasks;
restoration tasks; semisupervised classification;signal processing;
sparse representations; supervised dictionary learning;
task-driven dictionary learning;Cost function;Dictionaries;
Machine learning; Sensors;Sparse matrices;Vectors;Basis pursuit;Lasso;
compressed sensing.;dictionary learning; matrix factorization;
semi-supervised learning;Algorithms;Databases, Factual;Humans; 
Pattern Recognition, Automated},    
doi={10.1109/TPAMI.2011.156}, 
ISSN={0162-8828},}

@ARTICLE{Mammen1997,
  author = {Mammen, Enno and {Van De Geer}, Sara},
  title = {{Locally Adaptive Regression Splines}},
  journal = {Annals of Statistics},
  year = {1997},
  volume = {25},
  pages = {387--413},
  keywords = {nonparametric estimation; penalized least squares},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@article{Manolakis2001,
author = {Manolakis, Dimitris G. and Marden, David and Kerekes, John P. and Shaw, Gary A.},
title = {Statistics of hyperspectral imaging data},
journal = {Proceedings of the SPIE},
volume = {4381},
number = {},
pages = {308--316},
abstract = {Characterization of the joint (among wavebands) probability density function (PDF) of hyperspectral imaging (HSI) data is crucial for several applications, including the design of constant false alarm rate (CFAR) detectors and statistical classifiers. HSI data are vector (or equivalently multivariate) data in a vector space with dimension equal to the number of spectral bands. As a result, the scalar statistics utilized by many detection and classification algorithms depend upon the joint pdf of the data and the vector-to-scalar mapping defining the specific algorithm. For reasons of analytical tractability, the multivariate Gaussian assumption has dominated the development and evaluation of algorithms for detection and classification in HSI data, although it is widely recognized that it does not always provide an accurate model for the data. The purpose of this paper is to provide a detailed investigation of the joint and marginal distributional properties of HSI data. To this end, we assess how well the multivariate Gaussian pdf describes HSI data using univariate techniques for evaluating marginal normality, and techniques that use unidimensional views (projections) of multivariate data. We show that the class of elliptically contoured distributions, which includes the multivariate normal distribution as a special case, provides a better characterization of the data. Finally, it is demonstrated that the class of univariate stable random variables provides a better model for the heavy-tailed output distribution of the well known matched filter target detection algorithm.},
year = {2001},
doi = {10.1117/12.437021},
optURL = { http://dx.doi.org/10.1117/12.437021},
eprint = {}
}

@article{Manolakis2009,
  author = {Manolakis, D. and Lockwood, R. and Cooley, T. and Jacobson, J.},
  title = {Is there a best hyperspectral detection algorithm?},
  journal = {Proceedings of the SPIE},
  volume = {7334},
  number = {},
  pages = {733402-733402-16},
  abstract = {A large number of hyperspectral detection algorithms have been 
    developed and used over the last two decades. Some algorithms are based on
   highly sophisticated mathematical models and methods; others are derived 
   using intuition and simple geometrical concepts. The purpose of this paper
   is threefold. First, we discuss the key issues involved in the design and
   evaluation of detection algorithms for hyperspectral imaging data. Second, 
   we present a critical review of existing detection algorithms for practical
   hyperspectral imaging applications. Finally, we argue that the "apparent"  
   superiority of sophisticated algorithms with simulated data or in
   laboratory conditions, does not necessarily translate to superiority in
   real-world applications.}, 
year = {2009},
doi = {10.1117/12.816917},
optURL = { http://dx.doi.org/10.1117/12.816917},
eprint = {}
}

@MISC{Martinez2006,
  author = {Mart\'{\i}nez, Pablo J and P\'{e}rez, Rosa M and Plaza, Antonio and
	Aguilar, Pedro L and Cantero, Mar\'{\i}a C and Plaza, Javier},
  title = {{Endmember extraction algorithms from hyperspectral images}},
  year = {2006},
  note = {http://hdl.handle.net/2122/1963},
  abstract = {During the last years, several high-resolution sensors have been developed
	for hyperspectral remote sensing applications. Some of these sensors
	are already available on space-borne devices. Space-borne sensors
	are currently acquiring a continual stream of hyperspectral data,
	and new efficient unsupervised algorithms are required to analyze
	the great amount of data produced by these instruments. The identification
	of image endmembers is a crucial task in hyperspectral data exploitation.
	Once the individual endmembers have been identified, several methods
	can be used to map their spatial distribution, associations and abundances.
	This paper reviews the Pixel Purity Index (PPI), N-FINDR and Automatic
	Morphological Endmember Extraction (AMEE) algorithms developed to
	accomplish the task of finding appropriate image endmembers by applying
	them to real hyperspectral data. In order to compare the performance
	of these methods a metric based on the Root Mean Square Error (RMSE)
	between the estimated and reference abundance maps is used.},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mart\'{\i}nez et al. - 2006 - Endmember extraction algorithms from hyperspectral images.pdf:pdf},
  journal = {Annals of Geophysics},
  keywords = {endmember extraction,hyperspectral mixtures,linear spectral unmixing,},
  number = {1},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://hdl.handle.net/2122/1963},
  volume = {49}
}

@ARTICLE{Meinshausen2007,
  author = {Meinshausen, Nicolai},
  title = {{Relaxed Lasso}},
  journal = {Computational Statistics \& Data Analysis},
  year = {2007},
  volume = {52},
  pages = {374--393},
  number = {1},
  month = sep,
  doi = {10.1016/j.csda.2006.12.019},
  file = {:home/aepound/research/overcomplete/refs/toread/2007-RelaxedLasso-Meinshausen.pdf:pdf},
  issn = {01679473},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://linkinghub.elsevier.com/retrieve/pii/S0167947306004956}
}

@ARTICLE{Meinshausen2009,
  author = {Meinshausen, Nicolai and Yu, Bin},
  title = {{Lasso-type recovery of sparse representations for high-dimensional
	data}},
  journal = {The Annals of Statistics},
  year = {2009},
  volume = {37},
  pages = {246--270},
  number = {1},
  month = feb,
  doi = {10.1214/07-AOS582},
  file = {:home/aepound/research/overcomplete/refs/toread/2009-Lasso-typeRecoveryOnHighDimData-Meinshausen.pdf:pdf},
  issn = {0090-5364},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://projecteuclid.org/euclid.aos/1232115934}
}

@ARTICLE{Melgani2004,
  author = {Melgani, F. and Bruzzone, L.},
  title = {Classification of hyperspectral remote sensing images with support
	vector machines},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year = {2004},
  volume = {42},
  pages = {1778--1790},
  number = {8},
  month = aug,
  abstract = { This paper addresses the problem of the classification of hyperspectral
	remote sensing images by support vector machines (SVMs). First, we
	propose a theoretical discussion and experimental analysis aimed
	at understanding and assessing the potentialities of SVM classifiers
	in hyperdimensional feature spaces. Then, we assess the effectiveness
	of SVMs with respect to conventional feature-reduction-based approaches
	and their performances in hypersubspaces of various dimensionalities.
	To sustain such an analysis, the performances of SVMs are compared
	with those of two other nonparametric classifiers (i.e., radial basis
	function neural networks and the K-nearest neighbor classifier).
	Finally, we study the potentially critical issue of applying binary
	SVMs to multiclass problems in hyperspectral data. In particular,
	four different multiclass strategies are analyzed and compared: the
	one-against-all, the one-against-one, and two hierarchical tree-based
	strategies. Different performance indicators have been used to support
	our experimental studies in a detailed and accurate way, i.e., the
	classification accuracy, the computational time, the stability to
	parameter setting, and the complexity of the multiclass architecture.
	The results obtained on a real Airborne Visible/Infrared Imaging
	Spectroradiometer hyperspectral dataset allow to conclude that, whatever
	the multiclass strategy adopted, SVMs are a valid and effective alternative
	to conventional pattern recognition approaches (feature-reduction
	procedures combined with a classification method) for the classification
	of hyperspectral remote sensing data.},
  doi = {10.1109/TGRS.2004.831865},
  issn = {0196-2892},
  keywords = { Airborne Visible/Infrared Imaging Spectroradiometer hyperspectral
	dataset; Hughes phenomenon; K-nearest neighbor classifier; SVM classifiers;
	binary SVM; feature reduction; hierarchical tree-based strategies;
	hyperdimensional feature spaces; hyperspectral remote sensing images;
	hypersubspaces; image classification; multiclass architecture complexity;
	multiclass problems; multiclass strategies; nonparametric classifiers;
	pattern recognition; radial basis function neural networks; support
	vector machines; data acquisition; feature extraction; geophysical
	signal processing; image classification; multidimensional signal
	processing; remote sensing; spectral analysis; support vector machines;},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@INPROCEEDINGS{Moghaddam2008,
  author = {Moghaddam, Baback and Gruber, Amit and Weiss, Yair and Avidan, Shai},
  title = {{Sparse regression as a sparse eigenvalue problem}},
  booktitle = {Information Theory and Applications Workshop, 2008},
  year = {2008},
  pages = {219--225},
  month = jan,
  publisher = {Ieee},
  abstract = {We extend the l0-norm ldquosubspectralrdquo algorithms developed for
	sparse-LDA (Moghaddam, 2006) and sparse-PCA (Moghaddam, 2006) to
	more general quadratic costs such as MSE in linear (or kernel) regression.
	The resulting ldquosparse least squaresrdquo (SLS) problem is also
	NP-hard, by way of its equivalence to a rank-1 sparse eigenvalue
	problem. Specifically, for minimizing general quadratic cost functions
	we use a highly-efficient method for direct eigenvalue computation
	based on partitioned matrix inverse techniques that leads to times103
	speed-ups over standard eigenvalue decomposition. This increased
	efficiency mitigates the O(n4) complexity that limited the previous
	algorithmspsila utility for high-dimensional problems. Moreover,
	the new computation prioritizes the role of the less-myopic backward
	elimination stage which becomes even more efficient than forward
	selection. Similarly, branch-and-bound search for exact sparse least
	squares (ESLS) also benefits from partitioned matrix techniques.
	Our greedy sparse least squares (GSLS) algorithm generalizes Natarajanpsilas
	algorithm (Natarajan, 1995) also known as order-recursive matching
	pursuit (ORMP). Specifically, the forward pass of GSLS is exactly
	equivalent to ORMP but is more efficient, and by including the backward
	pass, which only doubles the computation, we can achieve a lower
	MSE than ORMP. In experimental comparisons with LARS (Efron, 2004),
	forward-GSLS is shown to be not only more efficient and accurate
	but more flexible in terms of choice of regularization.},
  doi = {10.1109/ITA.2008.4601051},
  file = {:home/aepound/research/overcomplete/refs/toread/2008-SparseRegressAsSparseEigProb-Moghaddam.pdf:pdf},
  isbn = {978-1-4244-2670-6},
  journal = {2008 Information Theory and Applications Workshop},
  keywords = {NP-hard problem;branch-and-bound search;eigenvalue decomposition;exact
	sparse least squares;greedy sparse least squares algorithm;less-myopic
	backward elimination stage;matrix inverse techniques;order-recursive
	matching pursuit;partitioned matrix techniques;quadratic cost functions;sparse
	eigenvalue problem;sparse least squares problem;sparse regression;subspectral
	algorithms;computational complexity;eigenvalues and eigenfunctions;iterative
	methods;least squares approximations;matrix inversion;sparse matrices;tree
	searching;},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4601051}
}
                  
@ARTICLE{Nascimento2005, 
author={Nascimento, J.M.P. and Bioucas Dias, J.M.}, 
journal={Geoscience and Remote Sensing, IEEE Transactions on}, 
title={Vertex component analysis: a fast algorithm to unmix hyperspectral data}, 
year={2005}, 
month=apr, 
volume={43}, 
number={4}, 
pages={898--910}, 
keywords={feature extraction;geophysical signal processing;geophysical techniques;image processing;multidimensional signal processing;remote sensing;abundance fractions;computational complexity;hyperspectral data unmixing;hyperspectral vectors;linear spectral mixture analysis;linear unmixing;mixed spectral vectors;multispectral vectors;spectral signatures;unsupervised endmember extraction;vertex component analysis;Algorithm design and analysis;Data mining;Hyperspectral imaging;Hyperspectral sensors;Independent component analysis;Least squares approximation;Pixel;Remote sensing;Scattering;Telecommunications;Linear unmixing;simplex;spectral mixture model;unmixing hypespectral data;unsupervised endmember extraction;vertex component analysis (VCA)}, 
doi={10.1109/TGRS.2005.844293}, 
ISSN={0196-2892},}
                  
@ARTICLE{Nasrabadi2014, 
author={Nasrabadi, N.M.}, 
journal={IEEE Signal Processing Magazine }, 
title={Hyperspectral Target Detection : An Overview of Current and Future Challenges}, 
year={2014}, 
month=jan, 
volume={31}, 
number={1}, 
pages={34--44}, 
keywords={hyperspectral imaging;infrared imaging;object detection;remote sensing;sensors;HSI;IR region;electromagnetic spectrum;hyperspectral imagery;hyperspectral scene;hyperspectral sensor measurement;hyperspectral spectrometer;hyperspectral target detection;remote sensing system;short-wave infrared region;spectral characteristics;target identification;visible region;Covariance matrices;Detectors;Hyperspectral imaging;Signal processing algorithms;Targeting}, 
doi={10.1109/MSP.2013.2278992}, 
ISSN={1053-5888}
}

@ARTICLE{Olshausen1997,
  author = {Olshausen, Bruno A and Field, David J.},
  title = {{Sparse coding with an overcomplete basis set: A strategy employed
	by V1?}},
  journal = {Vision Research},
  year = {1997},
  volume = {37},
  pages = {3311--3325},
  number = {23},
  month = dec,
  abstract = {The spatial receptive fields of simple cells in mammalian striate
	cortex have been reasonably well described physiologically and can
	be characterized as being localized, oriented, and bandpass, comparable
	with the basis functions of wavelet transforms. Previously, we have
	shown that these receptive field properties may be accounted for
	in terms of a strategy for producing a sparse distribution of output
	activity in response to natural images. Here, in addition to describing
	this work in a more expansive fashion, we examine the neurobiological
	implications of sparse coding. Of particular interest is the case
	when the code is overcompleteâi.e., when the number of code elements
	is greater than the effective dimensionality of the input space.
	Because the basis functions are non-orthogonal and not linearly independent
	of each other, sparsifying the code will recruit only those basis
	functions necessary for representing a given input, and so the input-output
	function will deviate from being purely linear. These deviations
	from linearity provide a potential explanation for the weak forms
	of non-linearity observed in the response properties of cortical
	simple cells, and they further make predictions about the expected
	interactions among units in response to naturalistic stimuli.},
  doi = {10.1016/S0042-6989(97)00169-7},
  file = {:home/aepound/research/overcomplete/refs/toread/1997-SparseCodingOvercompBases-Olshausen-Field.pdf:pdf},
  issn = {00426989},
  keywords = {Coding,Gabor-wavelet,Natural images,V1},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://linkinghub.elsevier.com/retrieve/pii/S0042698997001697}
}

@ARTICLE{Osborne2000,
  author = {Osborne, Michael R. and Presnell, Brett and Turlach, Berwin A.},
  title = {On the LASSO and Its Dual},
  journal = {Journal of Computational and Graphical Statistics},
  year = {2000},
  volume = {9},
  pages = {319--337},
  number = {2},
  abstract = {Proposed by Tibshirani, the least absolute shrinkage and selection
	operator (LASSO) estimates a vector of regression coefficients by
	minimizing the residual sum of squares subject to a constraint on
	the l<sup>1</sup>-norm of the coefficient vector. The LASSO estimator
	typically has one or more zero elements and thus shares characteristics
	of both shrinkage estimation and variable selection. In this article
	we treat the LASSO as a convex programming problem and derive its
	dual. Consideration of the primal and dual problems together leads
	to important new insights into the characteristics of the LASSO estimator
	and to an improved method for estimating its covariance matrix. Using
	these results we also develop an efficient algorithm for computing
	LASSO estimates which is usable even in cases where the number of
	regressors exceeds the number of observations. An S-Plus library
	based on this algorithm is available from StatLib.},
  copyright = {Copyright Â© 2000 American Statistical Association, Institute of Mathematical
	Statistics and Interface Foundation of America},
  issn = {10618600},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Jun., 2000},
  language = {English},
  owner = {aepound},
  publisher = {American Statistical Association, Institute of Mathematical Statistics,
	and Interface Foundation of America},
  timestamp = {2012.02.07},
  opturl = {http://www.jstor.org/stable/1390657}
}

@article{Osher2010,
  author = {Osher, Stanley and Yin, Wotao and Kelly, Kevin and Thiyanaratnam,
	Pradeep},
  title = {{Compressive Hyperspectral Imaging and Anomaly Detection}},
  year = {2010},
  abstract = {We have developed and tested state-of-the-art target detection/template
	matching methods based on L1 minimization. Given the spectral signature
	of a material, we are able to identify the pixels in a hyperspectral
	image, even for very noisy data, that contains the material. The
	speed of our unmixing algorithm is now much faster thatn any previous
	methods. We have also expanded the use of the Bayesian dictionary
	learning and sparse reconstruction method by utilizing spatial inter-relationships
	between different components in images and incorporating sparsity
	of spectral vectors in terms of sparse representation by endmembers
	into reconstruction.},
  booktitle = {Security},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Osher et al. - 2010 - Compressive Hyperspectral Imaging and Anomaly Detection.pdf:pdf},
  institution = {Air Force Office of Scientific Research},
  keywords = {Bayesian dictionary learning,L1,compressive,endmember,hyperspectral
	anomaly,sparse reconstruction,template matching,unmixing},
  owner = {aepound},
  pages = {1--19},
  timestamp = {2012.02.07},
  volume = {298}
}

@INBOOK{Plaza2010,
  pages = {235--268},
  title = {{Recent Developments in Endmember Extraction and Spectral Unmixing}},
  publisher = {Springer},
  year = {2010},
  editor = {Prasad, Saurabh and Bruce, Lori M. and Chanussot, Jocelyn},
  author = {Plaza, Antonio and Mart, Gabriel and Plaza, Javier and Zortea, Maciel
	and S, Sergio},
  edition = {1},
  booktitle = {Optical Remote Sensing - Advances in Signal Processing and Exploitation
	Techniques},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Plaza et al. - 2010 - Recent Developments in Endmember Extraction and Spectral Unmixing.pdf:pdf},
  isbn = {978-3-642-14211-6},
  keywords = {high spectral resolution,hyperspectral imagery possessing,multispectral
	imagery possessing,pre-processing images},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://www.springer.com/engineering/signals/book/978-3-642-14211-6}
}

@ARTICLE{Plaza2002,
  author = {Plaza, A. and Martinez, P. and Perez, R. and Plaza, J.},
  title = {{Spatial/spectral endmember extraction by multidimensional morphological
	operations}},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year = {2002},
  volume = {40},
  pages = {2025--2041},
  number = {9},
  month = sep,
  doi = {10.1109/TGRS.2002.802494},
  file = {:home/aepound/research/overcomplete/hsi/2002-SpatialSpectralEndmbrExtMultiDimMorph-Plaza-etal.pdf:pdf},
  issn = {0196-2892},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1046852}
}

@inproceedings{Raina2007,
 author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y.},
 title = {Self-taught Learning: Transfer Learning from Unlabeled Data},
 booktitle = {Proceedings of the 24th International Conference on Machine Learning},
 series = {ICML '07},
 year = {2007},
 isbn = {978-1-59593-793-3},
 location = {Corvalis, Oregon},
 OPTpages = {759--766},
 numpages = {8},
 opturl = {http://doi.acm.org/10.1145/1273496.1273592},
 doi = {10.1145/1273496.1273592},
 acmid = {1273592},
 publisher = {ACM},
 address = {New York},
} 

@article{rodarmel2002,
  title={Principal component analysis for hyperspectral image classification},
  author={Rodarmel, Craig and Shan, Jie},
  journal={Surveying and Land Information Science},
  volume={62},
  number={2},
  pages={115--122},
  year={2002},
  publisher={AMERICAN CONGRESS ON SURVEYING AND MAPPING}
}
                  
@ARTICLE{Roweis2000,
  author = {Roweis, Sam T. and Saul, Lawrence K.},
  title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
  journal = {Science},
  year = {2000},
  volume = {290},
  pages = {2323--2326},
  number = {5500},
  abstract = {Many areas of science depend on exploratory data analysis and visualization.
	The need to analyze large amounts of multivariate data raises the
	fundamental problem of dimensionality reduction: how to discover
	compact representations of high-dimensional data. Here, we introduce
	locally linear embedding (LLE), an unsupervised learning algorithm
	that computes low-dimensional, neighborhood-preserving embeddings
	of high-dimensional inputs. Unlike clustering methods for local dimensionality
	reduction, LLE maps its inputs into a single global coordinate system
	of lower dimensionality, and its optimizations do not involve local
	minima. By exploiting the local symmetries of linear reconstructions,
	LLE is able to learn the global structure of nonlinear manifolds,
	such as those generated by images of faces or documents of text.},
  doi = {10.1126/science.290.5500.2323},
  eprint = {http://www.sciencemag.org/content/290/5500/2323.full.pdf},
  keywords = {LLL; Local Linear Embedding; Dimension Reduction;},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://www.sciencemag.org/content/290/5500/2323.abstract}
}

@MISC{Rubinstein2008,
    author = {Ron Rubinstein and Michael Zibulevsky and Michael Elad},
    title = {Efficient Implementation of the K-SVD Algorithm using
	Batch Orthogonal Matching Pursuit}, 
    year = {2008}
} 

@book{Schott2007,
  title={Remote Sensing : The Image Chain Approach: The Image Chain Approach},
  author={Schott, J. R.},
  year={2007},
  publisher={Oxford University Press},
  address = {New York},
  edition = {2nd ed},
} 
				  
@ARTICLE{Schweizer2001,
  author = {Schweizer, S M and Moura, J F},
  title = {{Efficient detection in hyperspectral imagery.}},
  journal = {IEEE transactions on image processing : a publication of the IEEE
	Signal Processing Society},
  year = {2001},
  volume = {10},
  pages = {584--97},
  number = {4},
  month = jan,
  abstract = {Hyperspectral sensors collect hundreds of narrow and contiguously
	spaced spectral bands of data. Such sensors provide fully registered
	high resolution spatial and spectral images that are invaluable in
	discriminating between man-made objects and natural clutter backgrounds.
	The price paid for this high resolution data is extremely large data
	sets, several hundred of Mbytes for a single scene, that make storage
	and transmission difficult, thus requiring fast onboard processing
	techniques to reduce the data being transmitted. Attempts to apply
	traditional maximum likelihood detection techniques for in-flight
	processing of these massive amounts of hyperspectral data suffer
	from two limitations: first, they neglect the spatial correlation
	of the clutter by treating it as spatially white noise; second, their
	computational cost renders them prohibitive without significant data
	reduction like by grouping the spectral bands into clusters, with
	a consequent loss of spectral resolution. This paper presents a maximum
	likelihood detector that successfully confronts both problems: rather
	than ignoring the spatial and spectral correlations, our detector
	exploits them to its advantage; and it is computationally expedient,
	its complexity increasing only linearly with the number of spectral
	bands available. Our approach is based on a Gauss-Markov random field
	(GMRF) modeling of the clutter, which has the advantage of providing
	a direct parameterization of the inverse of the clutter covariance,
	the quantity of interest in the test statistic. We discuss in detail
	two alternative GMRF detectors: one based on a binary hypothesis
	approach, and the other on a "single" hypothesis formulation. We
	analyze extensively with real hyperspectral imagery data (HYDICE
	and SEBASS) the performance of the detectors, comparing them to a
	benchmark detector, the RX-algorithm. Our results show that the GMRF
	"single" hypothesis detector outperforms significantly in computational
	cost the RX-algorithm, while delivering noticeable detection performance
	improvement.},
  doi = {10.1109/83.913593},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schweizer, Moura - 2001 - Efficient detection in hyperspectral imagery..pdf:pdf},
  issn = {1057-7149},
  owner = {aepound},
  pmid = {18249648},
  timestamp = {2012.02.07},
  opturl = {http://www.ncbi.nlm.nih.gov/pubmed/18249648}
}

@article{Sigernes2012, 
author = {Fred Sigernes and Yuriy Ivanov and Sergey Chernouss and Trond Trondsen and Alexey Roldugin and Yury Fedorenko and Boris Kozelov and Andrey Kirillov and Ilia Kornilov and Vladimir Safargaleev and Silje Holmen and Margit Dyrland and Dag Lorentzen and Lisa Baddeley}, 
journal = {Opt. Express}, 
keywords = {Optical design of instruments; Spectroscopy, visible; Astronomical optics; Space optics; Calibration},
number = {25}, 
pages = {27650--27660}, 
publisher = {OSA},
title = {Hyperspectral all-sky imaging of auroras}, 
volume = {20}, 
month = dec,
year = {2012},
opturl = {http://www.opticsexpress.org/abstract.cfm?URI=oe-20-25-27650},
doi = {10.1364/OE.20.027650},
abstract = {A prototype auroral hyperspectral all-sky camera has been constructed and tested. It uses electro-optical tunable filters to image the night sky as a function of wavelength throughout the visible spectrum with no moving mechanical parts. The core optical system includes a new high power all-sky lens with F-number equal to f/1.1. The camera has been tested at the Kjell Henriksen Observatory (KHO) during the auroral season of 2011/2012. It detects all sub classes of aurora above ~\&\#x00BD; of the sub visual 1kR green intensity threshold at an exposure time of only one second. Supervised classification of the hyperspectral data shows promise as a new method to process and identify auroral forms.},
}

@ARTICLE{Sprechmann2011,
  author = {{Sprechmann}, P. and {Ramirez}, I. and {Sapiro}, G. and {Eldar},
	Y.~C.},
  title = {{C-HiLasso: A Collaborative Hierarchical Sparse Modeling Framework}},
  journal = {IEEE Transactions on Signal Processing},
  year = {2011},
  volume = {59},
  pages = {4183-4198},
  month = sep,
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2011ITSP...59.4183S},
  archiveprefix = {arXiv},
  doi = {10.1109/TSP.2011.2157912},
  eprint = {1006.1346},
  owner = {aepound},
  primaryclass = {stat.ML},
  timestamp = {2012.02.07}
}

@PHDTHESIS{Stites2011,
  author = {Stites, Matthew R},
  title = {{Utilization of a Probabilistic Model for Improved Hyperspectral
	Unmixing}},
  school = {Utah State University},
  year = {2011},
  address = {Logan, UT},
  file = {:home/aepound/research/overcomplete/hsi/2011-Proposal-Stites.pdf:pdf},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@ARTICLE{Tarabalka2010,
  author = {Tarabalka, Y. and Fauvel, M. and Chanussot, J. and Benediktsson,
	J.A.},
  title = {SVM- and MRF-Based Method for Accurate Classification of Hyperspectral
	Images},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  year = {2010},
  volume = {7},
  pages = {736--740},
  number = {4},
  month = oct,
  __markedentry = {[aepound:6]},
  abstract = {The high number of spectral bands acquired by hyperspectral sensors
	increases the capability to distinguish physical materials and objects,
	presenting new challenges to image analysis and classification. This
	letter presents a novel method for accurate spectral-spatial classification
	of hyperspectral images. The proposed technique consists of two steps.
	In the first step, a probabilistic support vector machine pixelwise
	classification of the hyperspectral image is applied. In the second
	step, spatial contextual information is used for refining the classification
	results obtained in the first step. This is achieved by means of
	a Markov random field regularization. Experimental results are presented
	for three hyperspectral airborne images and compared with those obtained
	by recently proposed advanced spectral-spatial classification techniques.
	The proposed method improves classification accuracies when compared
	to other classification approaches.},
  doi = {10.1109/LGRS.2010.2047711},
  issn = {1545-598X},
  keywords = {MRF;Markov random field regularization;SVM;classification accuracy;hyperspectral
	airborne images;hyperspectral images classification;hyperspectral
	sensors;image analysis;image classification;probabilistic support
	vector machine pixelwise classification;spatial contextual information;spectral
	bands;spectral-spatial classification;Markov processes;geophysical
	image processing;image classification;probability;support vector
	machines;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@ARTICLE{Tenenbaum2000,
  author = {Tenenbaum, Joshua B. and Silva, Vin de and Langford, John C.},
  title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
  journal = {Science},
  year = {2000},
  volume = {290},
  pages = {2319--2323},
  number = {5500},
  abstract = {Scientists working with large volumes of high-dimensional data, such
	as global climate patterns, stellar spectra, or human gene distributions,
	regularly confront the problem of dimensionality reduction: finding
	meaningful low-dimensional structures hidden in their high-dimensional
	observations. The human brain confronts the same problem in everyday
	perception, extracting from its high-dimensional sensory inputsÃ¢ÂÂ30,000
	auditory nerve fibers or 106 optic nerve fibersÃ¢ÂÂa manageably small
	number of perceptually relevant features. Here we describe an approach
	to solving dimensionality reduction problems that uses easily measured
	local metric information to learn the underlying global geometry
	of a data set. Unlike classical techniques such as principal component
	analysis (PCA) and multidimensional scaling (MDS), our approach is
	capable of discovering the nonlinear degrees of freedom that underlie
	complex natural observations, such as human handwriting or images
	of a face under different viewing conditions. In contrast to previous
	algorithms for nonlinear dimensionality reduction, ours efficiently
	computes a globally optimal solution, and, for an important class
	of data manifolds, is guaranteed to converge asymptotically to the
	true structure.},
  doi = {10.1126/science.290.5500.2319},
  eprint = {http://www.sciencemag.org/content/290/5500/2319.full.pdf},
  keywords = {ISOMAP; dimension reduction; high dimensionality;},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://www.sciencemag.org/content/290/5500/2319.abstract}
}
				  
@INPROCEEDINGS{Theiler2008, 
  author={Theiler, J. and Foy, B.R.}, 
  booktitle={IEEE International Geoscience and Remote Sensing Symposium},  
  title={EC-GLRT: Detecting Weak Plumes in Non-Gaussian Hyperspectral
    Clutter Using an Elliptically-Contoured Generalized Likelihood
	Ratio Test},  
  year={2008}, 
  month=jul, 
  volume={1}, 
  OPTpages={I-221--I-224}, 
  keywords={adaptive filters;atmospheric composition;clutter;matched
    filters;remote sensing;ACE;AMF;EC-GLRT detector;adaptive coherence
    estimator;adaptive matched filter;background clutter
    distribution;elliptically-contoured model;gaseous
	plumes;generalized likelihood ratio test;hyperspectral
	imagery;invariance properties;nonGaussian hyperspectral
	clutter;Chemical products;Coherence;Detectors;Hyperspectral
	imaging;Laboratories;Light rail systems;Matched filters;Maximum
	likelihood detection;Statistical distributions;Testing;chemical
	plume;generalized likelihood ratio test;hyperspectral
	imagery;matched filter;non-Gaussian distribution},   
  doi={10.1109/IGARSS.2008.4778833},
} 

@INPROCEEDINGS{Thomas2005,
  author = {Thomas, Owen},
  title = {{Target Confirmation in LWIR Hyperspectral Data using Neural Networks}},
  booktitle = {EMRS DTC Proceedings B19},
  year = {2005},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas - 2005 - Target Confirmation in LWIR Hyperspectral Data using Neural Networks.pdf:pdf},
  keywords = {alternating direction singular value,decomposition,multilayer perceptron},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@ARTICLE{Tibshirani1992,
  author = {Tibshirani, Robert},
  title = {{Principal curves revisited}},
  journal = {Statistics and Computing},
  year = {1992},
  volume = {2},
  pages = {183--190},
  number = {4},
  abstract = {A principal curve (Hastie and Stuetzle, 1989) is a smooth curve passing
	through the middle of a distribution or data cloud, and is a generalization
	of linear principal components. We give an alternative definition
	of a principal curve, based on a mixture model. Estimation is carried
	out through an EM algorithm. Some comparisons are made to the Hastie-Stuetzle
	definition.},
  doi = {10.1007/BF01889678},
  file = {:home/aepound/research/dimred/refs/princcurve\_revisited.pdf:pdf},
  issn = {09603174},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://www.springerlink.com/index/10.1007/BF01889678}
}

@ARTICLE{Tosic2011,
  author = {Tosic, Ivana and Frossard, Pascal},
  title = {Dictionary Learning},
  journal = {IEEE Signal Processing Magazine},
  year = {2011},
  volume = {28},
  pages = {27--38},
  number = {2},
  month = mar,
  abstract = {We describe methods for learning dictionaries that are appropriate
	for the representation of given classes of signals and multisensor
	data. We further show that dimensionality reduction based on dictionary
	representation can be extended to address specific tasks such as
	data analy sis or classification when the learning includes a class
	separability criteria in the objective function. The benefits of
	dictionary learning clearly show that a proper understanding of causes
	underlying the sensed world is key to task-specific representation
	of relevant information in high-dimensional data sets.},
  doi = {10.1109/MSP.2010.939537},
  file = {:home/aepound/research/overcomplete/refs/2011-DictionaryLearning-Tosic\_Frossard.pdf:pdf},
  issn = {1053-5888},
  keywords = {dictionaries,dictionary learning,engineering education,high dimensional
	data sets,multisensor data,objective function,relevant information,task-specific
	representation},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5714407}
}

@ARTICLE{Wang2010,
  author = {Su Wang and Chuin-Mu Wang and Mann-Li Chang and Ching-Tsorng Tsai
	and Chein-I Chang},
  title = {Applications of Kalman Filtering to Single Hyperspectral Signature
	Analysis},
  journal = {IEEE Sensors Journal},
  year = {2010},
  volume = {10},
  pages = {547--563},
  number = {3},
  month = mar,
  __markedentry = {[aepound:6]},
  abstract = {Kalman filter (KF) is a widely used statistical signal processing
	technique for parameter estimation. Recently, a KF-based approach
	to linear spectral unmixing, called KF-based linear spectral unmixing
	(KFLU) was developed for mixed pixel classification. However, its
	applicability to spectral characterization for spectral estimation,
	identification, and quantification has not been explored. This paper
	presents new applications of Kalman filtering in spectral estimation,
	identification and abundance quantification for which three KF-based
	spectral characterization signal processing techniques are developed.
	These techniques are completely different from the KFLU in the sense
	that the former performs a KF across a spectral coverage wavelength
	by wavelength as opposed to the latter, which implements a Kalman
	filter pixel vector by pixel vector throughout an entire image cube.
	In addition, the proposed KF-based techniques do not require a linear
	mixture model as KFLU does. Accordingly, they are not linear spectral
	unmixing methods, but rather spectral signature filters operating
	as if they are spectral measures.},
  doi = {10.1109/JSEN.2009.2038546},
  issn = {1530-437X},
  keywords = {KF-based linear spectral unmixing;Kalman filtering;abundance quantification;mixed
	pixel classification;parameter estimation;single hyperspectral signature
	analysis;spectral estimation;statistical signal processing technique;Kalman
	filters;parameter estimation;signal processing;statistical analysis;},
  owner = {aepound},
  timestamp = {2012.02.11}
}

@CONFERENCE{Winter1999,
  author = {Michael E. Winter},
  title = {{N-FINDR}: {A}n algorithm for fast autonomous spectral end-member determination
	in hyperspectral data},
  booktitle = {Proceedings of the SPIE},
  year = {1999},
  editor = {Michael R. Descour and Sylvia S. Shen},
  volume = {3753},
  number = {1},
  OPTpages = {266--275},
  OPTorganization = {SPIE},
  publisher = {SPIE},
  doi = {10.1117/12.366289},
  journal = {Imaging Spectrometry V},
  location = {Denver, CO, USA},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://link.aip.org/link/?PSI/3753/266/1}
}

@INPROCEEDINGS{Wright2008,
  author = {Wright, Stephen J and Nowak, Robert D and Figueiredo, Mario A T},
  title = {{Sparse reconstruction by separable approximation}},
  booktitle = {2008 IEEE International Conference on Acoustics, Speech and Signal
	Processing},
  year = {2008},
  number = {2},
  pages = {3373--3376},
  address = {Las Vegas, NV},
  month = mar,
  publisher = {IEEE},
  abstract = {Finding sparse approximate solutions to large underdetermined linear
	systems of equations is a common problem in signal/image processing
	and statistics. Basis pursuit, the least absolute shrinkage and selection
	operator (LASSO), wavelet-based deconvolution and reconstruction,
	and compressed sensing (CS) are a few well-known areas in which problems
	of this type appear. One standard approach is to minimize an objective
	function that includes a quadratic (pound 2) error term added to
	a sparsity-inducing (usually pound 1) regularizer. We present an
	algorithmic framework for the more general problem of minimizing
	the sum of a smooth convex function and a nonsmooth, possibly nonconvex,
	sparsity-inducing function. We propose iterative methods in which
	each step is an optimization subproblem involving a separable quadratic
	term (diagonal Hessian) plus the original sparsity-inducing term.
	Our approach is suitable for cases in which this subproblem can be
	solved much more rapidly than the original problem. In addition to
	solving the standard pound 2 - pound 1 case, our approach handles
	other problems, e.g., pound p regularizers with p ne 1, or group-separable
	(GS) regularizers. Experiments with CS problems show that our approach
	provides state-of-the-art speed for the standard pound 2 - pound
	1 problem, and is also efficient on problems with GS regularizers.},
  doi = {10.1109/ICASSP.2008.4518374},
  file = {:home/aepound/research/overcomplete/refs/toread/2008-SparseReconstructionSepApprox-Wright.pdf:pdf},
  isbn = {978-1-4244-1483-3},
  issn = {1520-6149},
  keywords = {Compressed sensing,Deconvolution,Equations,Image processing,Image
	reconstruction,Iterative algorithms,Iterative methods,Linear systems,Signal
	processing,Statistics},
  owner = {aepound},
  timestamp = {2012.02.07},
  opturl = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4518374}
}

@ARTICLE{Wright2009, 
author={Wright, J. and Yang, A.Y. and Ganesh, A. and Sastry, S.S. and Yi Ma}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Robust Face Recognition via Sparse Representation}, 
year={2009}, 
month=feb, 
volume={31}, 
number={2}, 
pages={210--227}, 
keywords={face recognition;feature extraction;lightning;object
recognition;random processes;regression analysis;signal
representation;Laplacianfaces;downsampled images;eigenfaces;feature
extraction;illumination;image-based object recognition;multiple linear
regression model;occlusion;random projections;robust face
recognition;sparse signal representation;Classification
algorithms;Face recognition;Feature extraction;Humans;Image
recognition;Lighting;Linear regression;Object
recognition;Robustness;Signal representations;Classifier design and
evaluation;Face and gesture recognition;Face recognition;Feature
evaluation and selection;Occlusion;Outlier rejection;Spare
representation;compressed sensing;ell^{1}--minimization;feature
extraction;occlusion and corruption;sparse representation;validation
and outlier rejection.;Algorithms;Artificial
Intelligence;Biometry;Cluster Analysis;Face;Humans;Image
Enhancement;Image Interpretation, Computer-Assisted;Pattern
Recognition, Automated;Reproducibility of Results;Sensitivity and
Specificity;Subtraction Technique},  
doi={10.1109/TPAMI.2008.79}, 
ISSN={0162-8828},}

@ARTICLE{Xing2011,
  author = {Xing, Zhengming and Zhou, Mingyuan and Castrodad, Alexey and Sapiro,
	Guillermo and Carin, Lawrence},
  title = {Dictionary Learning for Noisy and Incomplete Hyperspectral Images},
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  year = {2011},
  volume = {submitted},
  pages = {1--29},
  abstract = {We consider analysis of noisy and incomplete hyperspectral imagery,
	with the objective of removing the noise and inferring the missing
	data. The noise statistics may be wavelength-dependent, and the fraction
	of data missing (at random) may be substantial, including potentially
	entire bands, offering the potential to significantly reduce the
	quantity of data that need be measured. To achieve this objective,
	the imagery is divided into contiguous three-dimensional (3D) spatio-spectral
	blocks, of spatial dimension much less than the image dimension.
	It is assumed that each such 3D block may be represented as a linear
	combination of dictionary elements of the same dimension, plus noise,
	and the dictionary elements are learned in situ based on the observed
	data (no a priori training). The number of dictionary elements needed
	for representation of any particular block is typically small relative
	to the block dimensions, and all the image blocks are processed jointly
	(âcollaborativelyâ) to infer the underlying dictionary. We address
	dictionary learning from a Bayesian perspective, considering two
	distinct means of imposing sparse dictionary usage. These models
	allow inference of the number of dictionary elements needed as well
	as the underlying wavelength-dependent noise statistics. It is demonstrated
	that drawing the dictionary elements from a Gaussian process prior,
	imposing structure on the wavelength dependence of the dictionary
	elements, yields significant advantages, relative to the more-conventional
	approach of using an i.i.d. Gaussian prior for the dictionary elements;
	this advantage is particularly evident in the presence of noise.
	The framework is demonstrated by processing hyperspectral imagery
	with a significant number of voxels missing uniformly at random,
	with imagery at specific wavelengths missing entirely, and in the
	presence of substantial additive noise.},
  file = {:home/aepound/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xing et al. - 2011 - Dictionary Learning for Noisy and Incomplete Hyperspectral Images.pdf:pdf},
  hsialgorithm = {Dictionary},
  hsibands = {VNIR-SWIR},
  hsinotes = {Concerned with reconstruction of cube. Not endmember extraction or
	detection.},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@ARTICLE{Yaghoobi2009,
  author = {Yaghoobi, M. and Blumensath, T. and Davies, M.E.},
  title = {Dictionary Learning for Sparse Approximations With the Majorization
	Method},
  journal = {Signal Processing, IEEE Transactions on},
  year = {2009},
  volume = {57},
  pages = {2178--2191},
  number = {6},
  month = jun,
  doi = {10.1109/TSP.2009.2016257},
  issn = {1053-587X},
  keywords = {convergence method;dictionary learning;majorization method;optimization
	method;signal processing;sparse approximation;statistical estimation;approximation
	theory;convergence of numerical methods;dictionaries;optimisation;signal
	processing;sparse matrices;statistical analysis;},
  owner = {aepound},
  timestamp = {2012.02.07}
}

@PHDTHESIS{Zare2008,
  author = {Zare, Alina},
  title = {{Hyperspectral Endmember Detection and Band Selection Using Bayesian
	Methods}},
  school = {University of Florida},
  year = {2008},
  type = {PhD Dissertation},
  file = {:home/aepound/research/overcomplete/hsi/AZareDissertation.pdf:pdf},
  keywords = {Hyperspectral Imagery,endmember},
  owner = {aepound},
  pages = {1--139},
  timestamp = {2012.02.07}
}

@INPROCEEDINGS{Zhang2010,
  author = {Qiang Zhang and Baoxin Li},
  title = {Discriminative {K-SVD} for dictionary learning in face recognition},
  booktitle = {IEEE Conference
	on Computer Vision and Pattern Recognition (CVPR)},
  year = {2010},
  OPTpages = {2691--2698},
  month = jun,
  abstract = {In a sparse-representation-based face recognition scheme, the desired
	dictionary should have good representational power (i.e., being able
	to span the subspace of all faces) while supporting optimal discrimination
	of the classes (i.e., different human subjects). We propose a method
	to learn an over-complete dictionary that attempts to simultaneously
	achieve the above two goals. The proposed method, discriminative
	K-SVD (D-KSVD), is based on extending the K-SVD algorithm by incorporating
	the classification error into the objective function, thus allowing
	the performance of a linear classifier and the representational power
	of the dictionary being considered at the same time by the same optimization
	procedure. The D-KSVD algorithm finds the dictionary and solves for
	the classifier using a procedure derived from the K-SVD algorithm,
	which has proven efficiency and performance. This is in contrast
	to most existing work that relies on iteratively solving sub-problems
	with the hope of achieving the global optimal through iterative approximation.
	We evaluate the proposed method using two commonly-used face databases,
	the Extended YaleB database and the AR database, with detailed comparison
	to 3 alternative approaches, including the leading state-of-the-art
	in the literature. The experiments show that the proposed method
	outperforms these competing methods in most of the cases. Further,
	using Fisher criterion and dictionary incoherence, we also show that
	the learned dictionary and the corresponding classifier are indeed
	better-posed to support sparse-representation-based recognition.},
  doi = {10.1109/CVPR.2010.5539989},
  issn = {1063-6919},
  keywords = {AR database;D-KSVD;YaleB database;dictionary learning;discriminative
	K-SVD;linear classifier;optimization;over complete dictionary;sparse
	representation based face recognition;face recognition;learning (artificial
	intelligence);optimisation;pattern classification;},
  owner = {aepound},
  timestamp = {2012.02.16}
}

@ARTICLE{Zhao2007,
  author = {Zhao, Peng and Yu, Bin},
  title = {Stagewise Lasso},
  journal = {J. Mach. Learn. Res.},
  year = {2007},
  volume = {8},
  pages = {2701--2726},
  month = dec,
  acmid = {1390331},
  issn = {1532-4435},
  issue_date = {12/1/2007},
  numpages = {26},
  owner = {aepound},
  publisher = {JMLR.org},
  timestamp = {2012.02.07},
  opturl = {http://dl.acm.org/citation.cfm?id=1314498.1390331}
}

@book{MerriamWebster,
    author = {Merriam-Webster},
    howpublished = {Hardcover},
    isbn = {0877797080},
    keywords = {dictionary},
    publisher = {Merriam-Webster},
    title = {{Merriam-Webster's Collegiate Dictionary}},
    edition = {10},
    year = {2000}
}

@ARTICLE{Wang2013, 
author={Nan Wang and Bo Du and Liangpei Zhang}, 
journal={ IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
title={An Endmember Dissimilarity Constrained Non-Negative Matrix
Factorization Method for Hyperspectral Unmixing}, 
year={2013}, 
volume={6}, 
number={2}, 
pages={554--569}, 
keywords={geophysical image processing;hyperspectral imaging;matrix
decomposition;EDC;NMF;dataset space;endmember dissimilarity
constrained nonnegative matrix factorization method;endmember
dissimilarity constraint;hyperspectral unmixing;nonconvex
problem;smooth spectra;Hyperspectral imaging;Linear
programming;Minimization;Optimization;Vectors;Hyperspectral
imagery;linear mixture model;non-negative matrix
factorization;spectral unmixing},  
doi={10.1109/JSTARS.2013.2242255}, 
ISSN={1939-1404},}


@TECHREPORT{Criminisi2011,
  AUTHOR =       {Criminisi, A. and Shotton, J. and Konukoglu, E.},
  TITLE =        {Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning.},
  INSTITUTION =  {Microsoft Research},
  YEAR =         {2011},
  number =       {MSR-TR-2011-114},
  month =        oct
}

@article{Criminisi2012,
 author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
 title = {Decision Forests: A Unified Framework for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning},
 journal = {Foundations and Trends in Computer Graphics and Vision},
 issue_date = {February 2012},
 volume = {7},
 number = {2--3},
 month = feb,
 year = {2012},
 issn = {1572-2740},
 pages = {81--227},
 numpages = {147},
 opturl = {http://dx.doi.org/10.1561/0600000035},
 doi = {10.1561/0600000035},
 acmid = {2185838},
 publisher = {Now Publishers Inc.},
 address = {Hanover, MA, USA},
} 

@INPROCEEDINGS{Breiman1996,
    author = {Leo Breiman},
    title = {Bagging Predictors},
    booktitle = {Machine Learning},
    year = {1996},
    OPTpages = {123--140}
}

				  
@INPROCEEDINGS{Dietterich1991,
    author = {Thomas G. Dietterich and Ghulum Bakiri},
    title = {Error-Correcting Output Codes: A General Method for Improving Multiclass Inductive Learning Programs},
    booktitle = {IN PROCEEDINGS OF AAAI-91},
    year = {1991},
    pages = {572--577},
    publisher = {AAAI Press}
}
				  
@ARTICLE{Dietterich1995,
    author = {Thomas G. Dietterich and Ghulum Bakiri},
    title = {Solving multiclass learning problems via error-correcting output codes},
    journal = {Journal of Artificial Intelligence Research},
    year = {1995},
    volume = {2},
    pages = {263--286}
}
				  
@article{Shiraishi2011,
title = "Statistical approaches to combining binary classifiers for multi-class classification ",
journal = "Neurocomputing ",
volume = "74",
number = "5",
pages = {680--688},
year = "2011",
note = "",
issn = "0925-2312",
doi = "http://dx.doi.org/10.1016/j.neucom.2010.09.004",
opturl = "http://www.sciencedirect.com/science/article/pii/S0925231210003735",
author = "Yuichi Shiraishi and Kenji Fukumizu",
keywords = "Multi-class classification",
keywords = "Combining binary classifiers",
keywords = "Meta-learning",
keywords = "Stacking",
keywords = "Group lasso "
}

@article{Crammer2002,
 author = {Crammer, Koby and Singer, Yoram},
 title = {On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines},
 journal = {Journal of  Machine Learning Research},
 issue_date = {3/1/2002},
 volume = {2},
 month = mar,
 year = {2002},
 issn = {1532-4435},
 pages = {265--292},
 numpages = {28},
 opturl = {http://dl.acm.org/citation.cfm?id=944790.944813},
 acmid = {944813},
 publisher = {JMLR.org},
} 

@INPROCEEDINGS{Fauvel2007, 
author={Fauvel, M. and Chanussot, J. and Benediktsson, J.A. and Sveinsson, J.R.}, 
booktitle={IEEE International Geoscience and Remote Sensing Symposium, 2007},  
title={Spectral and spatial classification of hyperspectral data using
SVMs and morphological profiles},  
year={2007}, 
month=jul, 
OPTpages={4834--4837}, 
keywords={feature extraction;geomorphology;image classification;sensor
fusion;support vector machines;terrain mapping;ROSIS data;SVM;data
fusion;decision boundary feature extraction;hyperspectral
data;morphological profiles;pixel-wise classification;principal
components;spatial classification;spectral classification;support
vector machine classifier;urban areas;urban structures;Concatenated
codes;Feature extraction;Hyperspectral imaging;Personal communication
networks;Pixel;Spatial resolution;Support vector machine
classification;Support vector machines;Testing;Urban areas},  
doi={10.1109/IGARSS.2007.4423943},}

@techreport{Lawson2008,
  author      = {Janice Lawson and Jim Conger and Lee Balick},
  title       = {Investigation of Temporal Variation of LWIR Solid
  Signatures Using Denali},
  institution = {Lawrence Livermore National Laboratory, Los Alamos
  National Laboratory},
  year        = {2008},
  month       = may,
  number      = {LLNL-TR-403504},
}
				  
@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

				  
%#*******************************************************************
%# Medical HSI references...  General Surveys
%#*******************************************************************
@INPROCEEDINGS{Freeman1997a, 
  author={Freeman, J. and Downs, F. and Marcucci, L. and Lewis,
    E.N. and Blume, B. and Rish, J.},  
  booktitle={Engineering in Medicine and Biology Society,
    1997. Proceedings of the 19th Annual International Conference of
	the IEEE},  
  title={Multispectral and hyperspectral imaging: applications for 
    medical and surgical diagnostics},  
  year={1997}, 
  month=oct, 
  volume={2}, 
  pages={700--701}, 
  keywords={infrared imaging;medical image
    processing;surgery;algorithms;anatomy;background
    clutter;hyperspectral imaging;image processing
	methodologies;medical diagnostics;military;multispectral
    imaging;pathology;physiology;surgeon's vision extension;surgical 
    diagnostics;target classification;target detection;target
    identification;Biomedical imaging;Humans;Hyperspectral
	imaging;Image processing;Medical diagnostic imaging;Optical
	surface waves;Reflectivity;Surface topography;Surgery;Surges},   
  doi={10.1109/IEMBS.1997.757727}, 
  ISSN={1094-687X},
}

@article{Lu2014,
  author = {Lu, Guolan and Fei, Baowei},
  title = {Medical hyperspectral imaging: a review},
  journal = {Journal of Biomedical Optics},
  volume = {19},
  number = {1},
  pages = {010901},
  abstract = {Hyperspectral imaging (HSI) is an emerging imaging
    modality for medical applications, especially in disease diagnosis
    and image-guided surgery. HSI acquires a three-dimensional dataset
    called hypercube, with two spatial dimensions and one spectral
    dimension. Spatially resolved spectral imaging obtained by HSI
    provides diagnostic information about the tissue physiology,
    morphology, and composition. This review paper presents an overview
    of the literature on medical hyperspectral imaging technology and
    its applications. The aim of the survey is threefold: an
    introduction for those new to the field, an overview for those
    working in the field, and a reference for those searching for
    literature on a specific application.}, 
  year = {2014},
  isbn = {1083-3668},
  doi = {10.1117/1.JBO.19.1.010901},
  optURL = { http://dx.doi.org/10.1117/1.JBO.19.1.010901 },
  eprint = {}
}

@article{Li2013,
  title={Review of spectral imaging technology in biomedical
    engineering: achievements and challenges}, 
  author={Li, Qingli and He, Xiaofu and Wang, Yiting and Liu, Hongying
    and Xu, Dongrong and Guo, Fangmin}, 
  journal={Journal of biomedical optics},
  volume={18},
  number={10},
  pages={100901},
  year={2013},
  publisher={International Society for Optics and Photonics}
}

%#*****************************************************************************
%# Specific fields/areas of medical research
%#*****************************************************************************
%# General:
@inproceedings{Martin2006,
  title={Hyperspectral fluorescence imaging system for biomedical diagnostics},
  author={Martin, Matthew E and Wabuyele, Musundi B and Panjehpour,
    Masoud and Phan, Mary N and Overholt, Bergein F and Vo-Dinh, Tuan}, 
  booktitle={Biomedical Optics 2006},
  OPTpages={60800Q--60800Q},
  year={2006},
  organization={International Society for Optics and Photonics}
}

%# Specific:
@article{Blanco2012,
  author = {Blanco, Francisco and LÃÂ³pez-Mesas, Montserrat and
    Serranti, Silvia and Bonifazi, Giuseppe and Havel, Josef and
    Valiente, Manuel}, 
  title = {Hyperspectral imaging based method for fast
    characterization of kidney stone types}, 
  journal = {Journal of Biomedical Optics},
  volume = {17},
  number = {7},
  OPTpages = {076027-1--076027-12},
  year = {2012},
  isbn = {1083-3668},
  doi = {10.1117/1.JBO.17.7.076027},
  optURL = { http://dx.doi.org/10.1117/1.JBO.17.7.076027},
  eprint = {}
}

@article{Uhr2012,
  title = "Molecular profiling of individual tumor cells by hyperspectral microscopic imaging ",
  journal = "Translational Research ",
  volume = "159",
  number = "5",
  pages = {366--375},
  year = "2012",
  note = "",
  issn = "1931-5244",
  doi = "http://dx.doi.org/10.1016/j.trsl.2011.08.003",
  opturl = "http://www.sciencedirect.com/science/article/pii/S1931524411002635",
  author = "Jonathan W. Uhr and Michael L. Huebschman and Eugene
    P. Frenkel and Nancy L. Lane and Raheela Ashfaq and Huaying Liu and
    Dipen R. Rana and Lawrence Cheng and Alice T. Lin and Gareth
    A. Hughes and Xiaojing J. Zhang and Harold R. Garner" 
}

@inproceedings{Akbari2012,
  title={Detection of cancer metastasis using a novel macroscopic hyperspectral method},
  author={Akbari, Hamed and Halig, Luma V and Zhang, Hongzheng and
    Wang, Dongsheng and Chen, Zhuo G and Fei, Baowei}, 
  booktitle={SPIE Medical Imaging},
  OPTpages={831711--831711},
  year={2012},
  organization={International Society for Optics and Photonics}
}

@article{Panasyuk2007,
title = "Medical hyperspectral imaging to facilitate residual tumor identification during surgery",
journal = "Cancer Biology \& Therapy",
volume = "6",
number = "3",
pages = {439--446},
year = "2007",
note = "",
opturl = "http://www.landesbioscience.com/journals/cbt/article/4018/",
author = "Panasyuk, Svetlana V. and Yang, Shi and Faller, Douglas V. and Ngo, Duyen and 
Lew, Robert A. and Freeman, Jenny E. and Rogers, Adrianne E."
}

@article{Sorg2005,
  author = {Sorg, Brian S. and Moeller, Benjamin J. and Donovan, Owen
    and Cao, Yiting and Dewhirst,  Mark W.},
  title = {Hyperspectral imaging of hemoglobin saturation in tumor
    microvasculature and tumor hypoxia development}, 
  journal = {Journal of Biomedical Optics},
  volume = {10},
  number = {4},
  OPTpages = {044004-044004-11},
  abstract = {Tumor hypoxia has been shown to have prognostic value in
    clinical trials involving radiation, chemotherapy, and
    surgery. Tumor oxygenation studies at microvascular levels can
    provide understanding of oxygen transport on scales at which oxygen
    transfer to tissue occurs. To fully grasp the significance of blood
    oxygen delivery and hypoxia at microvascular levels during tumor
    growth and angiogenesis, the spatial and temporal relationship of
    the data must be preserved and mapped. Using tumors grown in window
    chamber models, hyperspectral imaging can provide serial spatial
    maps of blood oxygenation in terms of hemoglobin saturation at the
    microvascular level. We describe our application of hyperspectral
    imaging for in vivo microvascular tumor oxygen transport studies
    using red fluorescent protein (RFP) to identify all tumor cells, and
    hypoxia-driven green fluorescent protein (GFP) to identify the
    hypoxic fraction. 4T1 mouse mammary carcinoma cells, stably
    transfected with both reporter genes, are grown in dorsal skin-fold
    window chambers. Hyperspectral imaging is used to create image maps
    of hemoglobin saturation, and classify image pixels where RFP alone
    is present (tumor cells), or both RFP and GFP are present (hypoxic
    tumor cells). In this work, in vivo calibration of the imaging
    system is described and in vivo results are shown.}, 
  year = {2005},
  isbn = {1083-3668},
  doi = {10.1117/1.2003369},
optURL = { http://dx.doi.org/10.1117/1.2003369},
  eprint = {}
}

@article{siddiqi2008use,
  title={Use of hyperspectral imaging to distinguish normal, precancerous, and cancerous cells},
  author={Siddiqi, Anwer M and Li, Hui and Faruque, Fazlay and
    Williams, Worth and Lai, Kent and Hughson, Michael and Bigler,
    Steven and Beach, James and Johnson, William}, 
  journal={Cancer Cytopathology},
  volume={114},
  number={1},
  pages={13--21},
  year={2008},
  publisher={Wiley Online Library}
}

%#***********************************************
%# Geological references


@article{Gan2007,
  title={The application of the hyperspectral imaging technique to geological investigation},
  author={GAN, Fu-ping and WANG, Run-sheng},
  journal={Remote Sensing for Land \& Resources},
  volume={4},
  pages={014},
  year={2007}
}

@inproceedings{Winter1996,
  title={Experiments to support the development of techniques for hyperspectral mine detection},
  author={Winter, Edwin M and Schlangen, Michael J and Bowman, Anu P
  and Carter, Michael R and Bennett, Charles L and Fields, David J and
  Aimonetti, William D and Lucey, Paul G and Johnson, Jeannie and
  Horton, Keith A and Williams, Tim J. and Stocker, Alan D. and
  Oshagan, Ara and {DePersia}, A. Trent and Sayre, Craig J.}, 
  booktitle={Aerospace/Defense Sensing and Controls},
  OPTpages={139--148},
  year={1996},
  organization={International Society for Optics and Photonics}
}

@inproceedings{Kruse1998,
  title={Advances in hyperspectral remote sensing for geologic mapping and exploration},
  author={Kruse, Fred A},
  booktitle={Proceedings 9th Australasian Remote Sensing Conference, Sydney, Australia},
  year={1998}
}


@article{Resmini1997,
author = {Resmini, R. G. and Kappus, M. E. and Aldrich, W. S. and Harsanyi, J. C. and Anderson, M.},
title = {Mineral mapping with {HY}perspectral {D}igital {I}magery {C}ollection {E}xperiment ({HYDICE}) sensor data at {Cuprite, Nevada, U.S.A.}},
journal = {International Journal of Remote Sensing},
volume = {18},
number = {7},
pages = {1553--1570},
year = {1997},
doi = {10.1080/014311697218278},
optURL = { 
        http://dx.doi.org/10.1080/014311697218278    
},
eprint = { 
        http://dx.doi.org/10.1080/014311697218278
}
}

@article{vanderMeer2012,
title = "Multi- and hyperspectral geologic remote sensing: a review ",
journal = "International Journal of Applied Earth Observation and Geoinformation ",
volume = "14",
number = "1",
pages = {112--128},
year = "2012",
note = "",
issn = "0303-2434",
doi = "http://dx.doi.org/10.1016/j.jag.2011.08.002",
opturl = "http://www.sciencedirect.com/science/article/pii/S0303243411001103",
author = "Freek D. van der Meer and Harald M.A. van der Werff and Frank J.A. van Ruitenbeek and Chris A. Hecker and Wim H. Bakker and Marleen F. Noomen and Mark van der Meijde and E. John M. Carranza and J. Boudewijn de Smeth and Tsehaie Woldai",
keywords = "Geologic remote sensing",
keywords = "Landsat",
keywords = "\{ASTER\}",
keywords = "Hyperspectral "
}

@article{Sabins1999,
  title={Remote sensing for mineral exploration},
  author={Sabins, Floyd F},
  journal={Ore Geology Reviews},
  volume={14},
  number={3},
  pages={157--183},
  year={1999},
  publisher={Elsevier}
}

%#************************************************
%# Other applications
%# Military:
@article{Xu2007,
  title={Applications of multispectral/hyperspectral imaging technologies in military},
  author={Xu, Hong and Wang, Xiang-jun},
  journal={Infrared and Laser Engineering},
  volume={36},
  number={1},
  pages={13},
  year={2007},
  publisher={EDITORIAL BOARD OF JOURNAL OF INFRARED AND LASER ENGINEERING}
}
@techreport{Anderson1994,
  title={Military utility of multispectral and hyperspectral sensors},
  author={Anderson, R and Malila, W and Maxwell, R and Reed, L},
  year={1994},
  institution={Defense Technical Information Center Document}
}
@inproceedings{Ardouin2007,
  title={A demonstration of hyperspectral image exploitation for military applications},
  author={Ardouin, J-P and L{\'e}vesque, Jos{\'e}e and Rea, Terry A},
  booktitle={2007 10th International Conference on Information Fusion},
  OPTpages={1--8},
  year={2007},
  organization={IEEE}
}
%# Forestry
@article{Clark2005,
  title={Hyperspectral discrimination of tropical rain forest tree species at leaf to crown scales},
  author={Clark, Matthew L and Roberts, Dar A and Clark, David B},
  journal={Remote Sensing of Environment},
  volume={96},
  number={3},
  pages={375--398},
  year={2005},
  publisher={Elsevier}
}
@article{Govender2007,
  title={A review of hyperspectral remote sensing and its application in vegetation and water resource studies},
  author={Govender, M and Chetty, K and Bulcock, H},
  journal={Water Sa},
  volume={33},
  number={2},
  year={2007},
  publisher={Water Research Commission (WRC)}
}
                  @article{Ghiyamat2010,
  title={A review on hyperspectral remote sensing for homogeneous and heterogeneous forest biodiversity assessment},
  author={Ghiyamat, Azadeh and Shafri, Helmi ZM},
  journal={International Journal of Remote Sensing},
  volume={31},
  number={7},
  pages={1837--1856},
  year={2010},
  publisher={Taylor \& Francis}
}
                  
                  
                  
                  

